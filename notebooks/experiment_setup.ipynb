{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Distributions\n",
    "using GeometricIntegrators\n",
    "using Optim\n",
    "using Random\n",
    "using Flux\n",
    "using Enzyme\n",
    "using Zygote\n",
    "using Distances\n",
    "using Symbolics\n",
    "using Plots\n",
    "using RuntimeGeneratedFunctions\n",
    "using LinearAlgebra\n",
    "using DelimitedFiles\n",
    "RuntimeGeneratedFunctions.init(@__MODULE__)\n",
    "\n",
    "gr()\n",
    "\n",
    "_prod(a, b, c, arrs...) = a .* _prod(b, c, arrs...)\n",
    "_prod(a, b) = a .* b\n",
    "_prod(a) = a\n",
    "\n",
    "# generates a vector out of symbolic arrays (p,q) with a certain dimension\n",
    "function get_z_vector(dims)\n",
    "    @variables q[1:dims]\n",
    "    @variables p[1:dims]\n",
    "    z = vcat(q,p)\n",
    "    return z\n",
    "end\n",
    "\n",
    "# make combinations of bases of just the order that is given \n",
    "# e.g order = 2 will give just the bases whose powers sum to 2\n",
    "function poly_combos(z, order, inds...)\n",
    "    if order == 0\n",
    "        return Num[1]\n",
    "    elseif order == length(inds)\n",
    "        return [_prod([z[i] for i in inds]...)]\n",
    "    else\n",
    "        start_ind = length(inds) == 0 ? 1 : inds[end]\n",
    "        return vcat([poly_combos(z, order, inds..., j) for j in start_ind:length(z)]...)\n",
    "    end\n",
    "end\n",
    "\n",
    "# gives all bases monomials up to a certain order\n",
    "function primal_monomial_basis(z, order::Int)\n",
    "    return Vector{Symbolics.Num}(vcat([poly_combos(z, i) for i in 1:order]...))\n",
    "end\n",
    "\n",
    "# calculates coefficient bases up to a certain order\n",
    "# mostly for use with trigonometric functions example sin(k*z),\n",
    "# where k is the coefficient\n",
    "function primal_coeff_basis(z, max_coeff::Int)\n",
    "    return Vector{Symbolics.Num}(vcat([k .* z for k in 1:max_coeff]...))\n",
    "end\n",
    "\n",
    "# calculates +,-,*,/ between states as a new basis\n",
    "# the return output is a set to avoid duplicates\n",
    "function primal_operator_basis(z, operator)\n",
    "    return Vector{Symbolics.Num}([operator(z[i], z[j]) for i in 1:length(z)-1 for j in i+1:length(z)] ∪ [operator(z[j], z[i]) for i in 1:length(z)-1 for j in i+1:length(z)])\n",
    "end\n",
    "\n",
    "function primal_power_basis(z, max_power::Int)\n",
    "    if max_power > 0\n",
    "        return Vector{Symbolics.Num}(vcat([z.^i for i in 1:max_power]...))\n",
    "    elseif max_power < 0\n",
    "        return Vector{Symbolics.Num}(vcat([z.^-i for i in 1:abs(max_power)]...))\n",
    "    end\n",
    "end\n",
    "\n",
    "function polynomial_basis(z::Vector{Symbolics.Num} = get_z_vector(2); polyorder::Int = 0, operator=nothing, max_coeff::Int = 0)\n",
    "    primes = primal_monomial_basis(z, polyorder)\n",
    "    primes = vcat(primes, primal_coeff_basis(z, max_coeff))\n",
    "    if operator !== nothing\n",
    "        primes = vcat(primes, primal_operator_basis(z, operator))\n",
    "    end\n",
    "    return primes\n",
    "end\n",
    "\n",
    "function trigonometric_basis(z::Vector{Symbolics.Num} = get_z_vector(2), coeffs = nothing, polyorder::Int = 0, operator = nothing, max_coeff::Int = 0)\n",
    "    primes = polynomial_basis(z, polyorder = polyorder, operator = operator, max_coeff = max_coeff)\n",
    "    if coeffs != nothing\n",
    "    return vcat(sin.(collect(coeffs[1:Int(end/2)] .* primes)), cos.(collect(coeffs[Int(end/2)+1:end] .* primes)))\n",
    "    else \n",
    "        return vcat(sin.(primes), cos.(primes))\n",
    "    end\n",
    "end\n",
    "\n",
    "function exponential_basis(z::Vector{Symbolics.Num} = get_z_vector(2); polyorder::Int = 0, operator=nothing, max_coeff::Int = 0)\n",
    "    primes = polynomial_basis(z, polyorder = polyorder, operator = operator, max_coeff = max_coeff)\n",
    "    return exp.(primes)\n",
    "end\n",
    "\n",
    "function logarithmic_basis(z::Vector{Symbolics.Num} = get_z_vector(2); polyorder::Int = 0, operator=nothing, max_coeff::Int = 0)\n",
    "    primes = polynomial_basis(z, polyorder = polyorder, operator = operator, max_coeff = max_coeff)\n",
    "    return log.(abs.(primes))\n",
    "end\n",
    "\n",
    "function mixed_states_basis(basis::Vector{Symbolics.Num}...)\n",
    "    mixed_states = Tuple(basis)\n",
    "    \n",
    "    ham = Vector{Symbolics.Num}()\n",
    "    for i in eachindex(mixed_states)\n",
    "        for j in i+1:lastindex(mixed_states)\n",
    "            ham = vcat(ham, [mixed_states[i][k] * mixed_states[j][l] for k in 1:length(mixed_states[i]) for l in 1:length(mixed_states[j])])\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return Vector{Symbolics.Num}(ham)\n",
    "end\n",
    "\n",
    "# returns the number of required coefficients for the basis\n",
    "function get_numCoeffs(basis::Vector{Symbolics.Num})\n",
    "    return length(basis)\n",
    "end\n",
    "\n",
    "#TODO: maybe basis shouldn't be variable number of arguments\n",
    "# gets a vector of combinations of hamiltonian basis\n",
    "function get_basis_set(basis::Vector{Symbolics.Num}...)\n",
    "    # gets a vector of combinations of basis\n",
    "    basis = vcat(basis...)\n",
    "    \n",
    "    # removes duplicates\n",
    "    basis = Vector{Symbolics.Num}(collect(unique(basis)))\n",
    "\n",
    "    return basis\n",
    "end\n",
    "\n",
    "mutable struct HamiltonianSINDy{T, GHT}\n",
    "    basis::Vector{Symbolics.Num} # the augmented basis for sparsification\n",
    "    analytical_fθ::GHT\n",
    "    z::Vector{Symbolics.Num} \n",
    "    λ::T # Sparsification Parameter\n",
    "    noise_level::T # Noise amplitude added to the data\n",
    "    noiseGen_timeStep::T # Time step for the integrator to get noisy data \n",
    "    nloops::Int # Sparsification Loops\n",
    "    batch_size::Int # Batch size for training\n",
    "    basis_coeff::Float64 # Coefficient for the coefficients of the basis\n",
    "    \n",
    "    function HamiltonianSINDy(basis::Vector{Symbolics.Num},\n",
    "        analytical_fθ::GHT = missing,\n",
    "        z::Vector{Symbolics.Num} = get_z_vector(2);\n",
    "        λ::T = 0.05,\n",
    "        noise_level::T = 0.00,\n",
    "        noiseGen_timeStep::T = 0.05,\n",
    "        nloops::Int = 10,\n",
    "        batch_size::Int = 1,\n",
    "        basis_coeff::Float64) where {T, GHT <: Union{Base.Callable,Missing}}\n",
    "\n",
    "        new{T, GHT}(basis, analytical_fθ, z, λ, noise_level, noiseGen_timeStep, nloops, batch_size::Int, basis_coeff::Float64)\n",
    "    end\n",
    "end\n",
    "\n",
    "function gen_noisy_ref_data(method::HamiltonianSINDy, x)\n",
    "    # initialize timestep data for analytical solution\n",
    "    tstep = method.noiseGen_timeStep\n",
    "    tspan = (zero(tstep), tstep)\n",
    "\n",
    "    function next_timestep(x)\n",
    "        prob_ref = ODEProblem((dx, t, x, params) -> method.analytical_fθ(dx, x, params, t), tspan, tstep, x)\n",
    "        sol = integrate(prob_ref, Gauss(2))\n",
    "        sol.q[end]\n",
    "    end\n",
    "\n",
    "    data_ref = [next_timestep(_x) for _x in x]\n",
    "\n",
    "    # add noise\n",
    "    data_ref_noisy = [_x .+ method.noise_level .* randn(size(_x)) for _x in data_ref]\n",
    "\n",
    "    return data_ref_noisy\n",
    "\n",
    "end\n",
    "\n",
    "struct TrainingData{AT<:AbstractArray}\n",
    "    x::AT # initial condition\n",
    "    ẋ::AT # initial condition\n",
    "    y::AT # noisy data at next time step\n",
    "\n",
    "    TrainingData(x::AT, ẋ::AT, y::AT) where {AT} = new{AT}(x, ẋ, y)\n",
    "    TrainingData(x::AT, ẋ::AT) where {AT} = new{AT}(x, ẋ)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trig_B7_basis (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function trig_B6_basis(a)\n",
    "    z = get_z_vector(2)\n",
    "    out = []\n",
    "    out = vcat(out, sum(collect(a[1:2] .* z[1:2])))\n",
    "    out = vcat(out, sum(collect(a[3:4] .* z[1:2])))\n",
    "    out = vcat(out, sum(collect(a[1:2] .* z[3:4])))\n",
    "    out = vcat(out, sum(collect(a[3:4] .* z[3:4])))\n",
    "    out = vcat(out, sum(collect(a[1:2] .* z[1:2])).^2)\n",
    "    out = vcat(out, sum(collect(a[3:4] .* z[1:2])).^2)\n",
    "    out = vcat(out, sum(collect(a[1:2] .* z[3:4])).^2)\n",
    "    out = vcat(out, sum(collect(a[3:4] .* z[3:4])).^2)\n",
    "    out = vcat(out, sin.(sum(collect(a[1:2] .* (z[1:2])))))\n",
    "    out = vcat(out, sin.(sum(collect(a[3:4] .* (z[1:2])))))\n",
    "    out = vcat(out, cos.(sum(collect(a[1:2] .* (z[1:2])))))\n",
    "    out = vcat(out, cos.(sum(collect(a[3:4] .* (z[1:2])))))\n",
    "    out = vcat(out, sin.(sum(collect(a[1:2] .* (z[3:4])))))\n",
    "    out = vcat(out, sin.(sum(collect(a[3:4] .* (z[3:4])))))\n",
    "    out = vcat(out, cos.(sum(collect(a[1:2] .* (z[3:4])))))\n",
    "    out = vcat(out, cos.(sum(collect(a[3:4] .* (z[3:4])))))\n",
    "    Vector{Symbolics.Num}(out)\n",
    "end\n",
    "\n",
    "function trig_B7_basis(a)\n",
    "    z = get_z_vector(2)\n",
    "    out = []\n",
    "    out = vcat(out, sum(collect(a[1:2] .* z[1:2])))\n",
    "    out = vcat(out, sum(collect(a[3:4] .* z[1:2])))\n",
    "    out = vcat(out, sum(collect(a[5:6] .* z[3:4])))\n",
    "    out = vcat(out, sum(collect(a[7:8] .* z[3:4])))\n",
    "    out = vcat(out, sum(collect(a[1:2] .* z[1:2])).^2)\n",
    "    out = vcat(out, sum(collect(a[3:4] .* z[1:2])).^2)\n",
    "    out = vcat(out, sum(collect(a[5:6] .* z[3:4])).^2)\n",
    "    out = vcat(out, sum(collect(a[7:8] .* z[3:4])).^2)\n",
    "    out = vcat(out, sin.(sum(collect(a[1:2] .* (z[1:2])))))\n",
    "    out = vcat(out, sin.(sum(collect(a[3:4] .* (z[1:2])))))\n",
    "    out = vcat(out, cos.(sum(collect(a[1:2] .* (z[1:2])))))\n",
    "    out = vcat(out, cos.(sum(collect(a[3:4] .* (z[1:2])))))\n",
    "    out = vcat(out, sin.(sum(collect(a[5:6] .* (z[3:4])))))\n",
    "    out = vcat(out, sin.(sum(collect(a[7:8] .* (z[3:4])))))\n",
    "    out = vcat(out, cos.(sum(collect(a[5:6] .* (z[3:4])))))\n",
    "    out = vcat(out, cos.(sum(collect(a[7:8] .* (z[3:4])))))\n",
    "    Vector{Symbolics.Num}(out)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "basis_func_maker (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function poly_basis_maker(z, nd, polyorder)\n",
    "    \n",
    "    polynomial = polynomial_basis(z, polyorder=polyorder)\n",
    "    poly_basis = get_basis_set(polynomial)\n",
    "    return poly_basis\n",
    "end\n",
    "\n",
    "function B3(z, d, trig_polyorder, basis)\n",
    "    total_trig_args = 2*(binomial(2d + trig_polyorder, trig_polyorder) - 1) # multiply by 2 for sin and cos terms\n",
    "    @variables a[1:length(basis)+2*total_trig_args] # multiply by 2 to also account for amplitude coefficients\n",
    "    trig_basis = trigonometric_basis(z, a[1:total_trig_args], trig_polyorder)\n",
    "    poly_trig_basis = polynomial_basis(trig_basis, polyorder = 1)\n",
    "    basis = get_basis_set(basis, poly_trig_basis)\n",
    "    return basis, a, total_trig_args\n",
    "end\n",
    "\n",
    "function B4(z, d, trig_polyorder, basis)\n",
    "    num_trig_arg = 2*(binomial(2d + trig_polyorder, trig_polyorder) - 1) # multiply by 2 to get coefficients for both sin and cos bases\n",
    "\n",
    "    # code for half trig argument bases\n",
    "    halfz = [(z[1]+z[2])/2, (z[3]+z[4])/2]\n",
    "    num_half_trig_args = 2*length(halfz) # multiply by 2 to get coefficients for both sin and cos bases\n",
    "\n",
    "    total_trig_args = num_trig_arg + num_half_trig_args\n",
    "\n",
    "    # basis for fractional polynomial bases up to the second power (without variable mixing)\n",
    "    half_poly_basis = get_basis_set(halfz, halfz.^2)\n",
    "    \n",
    "    # collects and sums combinations of basis and coefficients  \n",
    "    @variables a[1:2*total_trig_args+length(basis)+length(half_poly_basis)] # use multiply by 2 to account for amplitude coefficients also\n",
    "\n",
    "    # code for trig argument bases\n",
    "    trig_basis = trigonometric_basis(z, a[1:num_trig_arg], trig_polyorder)\n",
    "    poly_trig_basis = polynomial_basis(trig_basis, polyorder = 1)\n",
    "\n",
    "    half_trig_basis = trigonometric_basis(halfz, a[num_trig_arg+1:total_trig_args], trig_polyorder)\n",
    "    poly_halfTrig_basis = polynomial_basis(half_trig_basis, polyorder = 1)\n",
    "\n",
    "    # basis = get_basis_set(basis, poly_trig_basis)\n",
    "    basis = get_basis_set(basis, half_poly_basis, poly_trig_basis, poly_halfTrig_basis)\n",
    "    return basis, a, total_trig_args\n",
    "end\n",
    "\n",
    "function B5(z, d, trig_polyorder, basis)\n",
    "    num_trig_arg = 2*(binomial(2d + trig_polyorder, trig_polyorder) - 1) # multiply by 2 to get coefficients for both sin and cos bases\n",
    "    num_half_trig_args  = 12 # get argument coefficients for both sin and cos bases\n",
    "    total_trig_args = num_trig_arg + num_half_trig_args\n",
    "\n",
    "    # code for half trig argument bases\n",
    "    halfz = [(z[1]+z[2])/2, (z[1]+z[3])/2, (z[1]+z[4])/2, (z[2]+z[3])/2, (z[2]+z[4])/2, (z[3]+z[4])/2]\n",
    "    half_poly_basis = get_basis_set(halfz, halfz.^2)\n",
    "\n",
    "    # collects and sums combinations of basis and coefficients  \n",
    "    @variables a[1:2*total_trig_args+length(basis)+length(half_poly_basis)] # multiply by trig_args by 2 to account for amplitude coefficients also\n",
    "\n",
    "    # code for trig argument bases\n",
    "    trig_basis = trigonometric_basis(z, a[1:num_trig_arg], trig_polyorder)\n",
    "    poly_trig_basis = polynomial_basis(trig_basis, polyorder = 1)\n",
    "\n",
    "    # get trigonometric bases from halfz basis\n",
    "    half_trig_basis = trigonometric_basis(halfz, a[num_trig_arg+1:total_trig_args], trig_polyorder)\n",
    "    poly_halfTrig_basis = polynomial_basis(half_trig_basis, polyorder = 1)\n",
    "\n",
    "    basis = get_basis_set(basis, half_poly_basis, poly_trig_basis, poly_halfTrig_basis)\n",
    "    return basis, a, total_trig_args\n",
    "end\n",
    "\n",
    "function B6(z, d, trig_polyorder, basis)\n",
    "    basis_one_trig_args = 2*(binomial(2d + trig_polyorder, trig_polyorder) - 1) # multiply by 2 for sin and cos terms\n",
    "    basis_six_trig_args = 2d\n",
    "    total_trig_args = basis_one_trig_args + basis_six_trig_args\n",
    "\n",
    "    basis_six_amplitude_coeffs = 16\n",
    "\n",
    "    @variables a[1:length(basis) + 2*basis_one_trig_args + basis_six_amplitude_coeffs + basis_six_trig_args] # 2*basis_one_trig_args multiply by 2 to account for amplitude coefficients\n",
    "    \n",
    "    trig_basis = trig_B6_basis(a)\n",
    "    basis_one_trig_basis = trigonometric_basis(z, a[basis_six_trig_args+1:total_trig_args], trig_polyorder)\n",
    "\n",
    "    basis = get_basis_set(basis, basis_one_trig_basis, trig_basis)\n",
    "    return basis, a, total_trig_args\n",
    "end\n",
    "\n",
    "function B7(z, d, trig_polyorder, basis)\n",
    "    basis_one_trig_args = 2*(binomial(2d + trig_polyorder, trig_polyorder) - 1) # multiply by 2 for sin and cos terms\n",
    "    basis_six_trig_args = 4d\n",
    "    total_trig_args = basis_one_trig_args + basis_six_trig_args\n",
    "\n",
    "    basis_six_amplitude_coeffs = 16\n",
    "\n",
    "    @variables a[1:length(basis) + 2*basis_one_trig_args + basis_six_amplitude_coeffs + basis_six_trig_args] # 2*basis_one_trig_args multiply by 2 to account for amplitude coefficients\n",
    "    \n",
    "    trig_basis = trig_B7_basis(a)\n",
    "    basis_one_trig_basis = trigonometric_basis(z, a[basis_six_trig_args+1:total_trig_args], trig_polyorder)\n",
    "    \n",
    "    basis = get_basis_set(basis, basis_one_trig_basis, trig_basis)\n",
    "    return basis, a, total_trig_args\n",
    "end\n",
    "\n",
    "# Wrapper for selecting the correct trig basis function\n",
    "function basis_func_maker(trig_basis_case, z, d, trig_polyorder, poly_basis)\n",
    "    if trig_basis_case == 1 || trig_basis_case == 2 || trig_basis_case == 3\n",
    "        return B3(z, d, trig_polyorder, poly_basis)\n",
    "    elseif trig_basis_case == 4\n",
    "        return B4(z, d, trig_polyorder, poly_basis)\n",
    "    elseif trig_basis_case == 5\n",
    "        return B5(z, d, trig_polyorder, poly_basis)\n",
    "    elseif trig_basis_case == 6\n",
    "        return B6(z, d, trig_polyorder, poly_basis)\n",
    "    elseif trig_basis_case == 7\n",
    "        return B7(z, d, trig_polyorder, poly_basis)\n",
    "    else\n",
    "        error(\"case can be integer from 1 to 7 only\")\n",
    "    end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ΔH_func_builder (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# returns a function that can build the augmented gradient of the hamiltonian\n",
    "function ΔH_func_builder(d::Int, z::Vector{Symbolics.Num}, poly_basis::Vector{Symbolics.Num}, trig_polyorder::Int, trig_basis_case::Int) \n",
    "    # nd is the total number of dimensions of all the states, e.g. if q,p each of 3 dims, that is 6 dims in total\n",
    "    nd = 2d\n",
    "    Dz = Differential.(z)\n",
    "\n",
    "    basis, a, total_trig_args =  basis_func_maker(trig_basis_case, z, d, trig_polyorder, poly_basis)\n",
    "    \n",
    "    # collect and sum combinations of basis and coefficients\n",
    "    ham = sum(collect(a[total_trig_args+1:end] .* basis)) # start count of (a) coefficients from after the number of trig arguments\n",
    "    \n",
    "    # gives derivative of the hamiltonian, but not the skew-symmetric true one\n",
    "    f = [expand_derivatives(dz(ham)) for dz in Dz]\n",
    "\n",
    "    # simplify the expression potentially to make it faster\n",
    "    f = simplify(f)\n",
    "    \n",
    "    # line below makes the vector into a hamiltonian vector field by multiplying with the skew-symmetric matrix\n",
    "    ∇H = vcat(f[d+1:2d], -f[1:d])\n",
    "    \n",
    "    # builds a function that calculates Hamiltonian gradient and converts the function to a native Julia function\n",
    "    ∇H_eval = @RuntimeGeneratedFunction(Symbolics.inject_registered_module_functions(build_function(∇H, z, a)[2]))\n",
    "    \n",
    "    return ∇H_eval\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generate_training_data (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# two-dim simple harmonic oscillator (not used anywhere only in case some testing needed)\n",
    "# H_ana(x, p, t) = ϵ * x[1]^2 + ϵ * x[2]^2 + 1/(2*m) * x[3]^2 + 1/(2*m) * x[4]^2\n",
    "# H_ana(x, p, t) = cos(x[1]) + cos(x[2]) + 1/(2*m) * x[3]^2 + 1/(2*m) * x[4]^2\n",
    "\n",
    "# Gradient function of the 2D hamiltonian\n",
    "# grad_H_ana(x) = [x[3]; x[4]; -2ϵ * x[1]; -2ϵ * x[2]]\n",
    "grad_H_ana(x) = [x[3]; x[4]; sin(x[1]); sin(x[2])]\n",
    "function grad_H_ana!(dx, x, p, t)\n",
    "    dx .= grad_H_ana(x)\n",
    "end\n",
    "\n",
    "function generate_training_data(num_samp, input_range, nd)\n",
    "    # samples in p and q space\n",
    "    samp_range = LinRange(-input_range, input_range, num_samp)\n",
    "\n",
    "    # s depend on size of nd (total dims), 4 in the case here so we use samp_range x samp_range x samp_range x samp_range\n",
    "    s = collect(Iterators.product(fill(samp_range, nd)...))\n",
    "\n",
    "    # compute vector field from x state values\n",
    "    x = [collect(s[i]) for i in eachindex(s)]\n",
    "\n",
    "    dx = zeros(nd)\n",
    "    p = 0\n",
    "    t = 0\n",
    "    ẋ = [grad_H_ana!(copy(dx), _x, p, t) for _x in x]\n",
    "\n",
    "    # Change to matrices for faster computations\n",
    "    x = hcat(x...)\n",
    "    ẋ = hcat(ẋ...)\n",
    "    tdata = TrainingData(Float32.(x), Float32.(ẋ))\n",
    "    return tdata\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define all training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function batched_jacobian(model_layer, x_batch)\n",
    "    output_dim = size(model_layer(x_batch[:, 1]))[1]\n",
    "    batch_size = size(x_batch, 2)\n",
    "    \n",
    "    batch_jac = zeros(output_dim, batch_size, size(x_batch, 1))\n",
    "    \n",
    "    for i in 1:batch_size\n",
    "        x_input = x_batch[:, i]\n",
    "        jac = Flux.jacobian(model_layer, x_input)[1]\n",
    "        batch_jac[:, i, :] = jac\n",
    "    end\n",
    "    return batch_jac\n",
    "end\n",
    "\n",
    "# Get ż from dz/dx and ẋ\n",
    "function enc_ż(enc_jac_batch, ẋ_batch)\n",
    "    ż_ref = zero(ẋ_batch)\n",
    "    for i in 1:size(enc_jac_batch, 2)\n",
    "        ż_ref[:, i] = enc_jac_batch[:,i,:] * (ẋ_batch[:,i])\n",
    "    end\n",
    "    return ż_ref\n",
    "end\n",
    "\n",
    "function evaluate_fθ(fθ, enc_x_batch, coeffs)\n",
    "    f = zero(enc_x_batch[:,1])\n",
    "    out = zero(enc_x_batch)\n",
    "    for i in 1:size(enc_x_batch, 2)\n",
    "        fθ(f, enc_x_batch[:,i], coeffs)\n",
    "        out[:,i] = f\n",
    "    end\n",
    "    return out\n",
    "end\n",
    "\n",
    "# Get ẋ from decoder derivative (dx/dz) and ż\n",
    "function dec_ẋ(dec_jac_batch, ż)\n",
    "    dec_mult_ẋ = zero(ż)\n",
    "    for i in 1:size(dec_jac_batch, 2)\n",
    "        dec_mult_ẋ[:, i] = dec_jac_batch[:,i,:] * ż[:,i]\n",
    "    end\n",
    "    return dec_mult_ẋ\n",
    "end\n",
    "\n",
    "function Diff_ż(grad_fθ, ż_ref)\n",
    "    return sum(abs2, (grad_fθ - ż_ref))\n",
    "end\n",
    "\n",
    "function Diff_ẋ(dec_jac_batch, grad_fθ, ẋ_ref)\n",
    "    ẋ_SINDy = zero(ẋ_ref)\n",
    "    for i in 1:size(dec_jac_batch, 2)\n",
    "        ẋ_SINDy[:, i] = dec_jac_batch[:,i,:] * grad_fθ[:,i]\n",
    "    end\n",
    "    return sum(abs2, (ẋ_SINDy - ẋ_ref))\n",
    "end\n",
    "\n",
    "# set the values of the model coeffs to the values of the Ξ vector for each state\n",
    "function set_coeffs(model_Coeffs, Ξ, biginds)\n",
    "    coeffs = zero(model_Coeffs)\n",
    "    coeffs[biginds] = Ξ\n",
    "    return coeffs\n",
    "end\n",
    "\n",
    "function loss(model, x_batch, ẋ_batch, ż_ref_batch, dec_jac_batch, alphas, basis_coeff)\n",
    "    enc_x_batch = model[1].W(x_batch)\n",
    "\n",
    "    coeffs = model[3].W\n",
    "\n",
    "    # Compute the reconstruction loss for the entire batch\n",
    "    L_r = sum(abs2, model[2].W(enc_x_batch) - x_batch)\n",
    "\n",
    "    # Note: grad_fθ, dec_mult_ẋ, and L_c in loss function so model acts on terms in loss function\n",
    "    # and gradient can see that and use that for its update calculations\n",
    "\n",
    "    # encoded gradient from SINDy\n",
    "    grad_fθ = evaluate_fθ(fθ, enc_x_batch, coeffs)\n",
    "\n",
    "    # Difference b/w encoded gradients from SINDy and reference\n",
    "    #TODO: could also try alphas/10 instead of alphas/100\n",
    "    L_ż = alphas / 100 * Diff_ż(grad_fθ, ż_ref_batch)\n",
    "\n",
    "    # Difference b/w decoded-encoded gradients from SINDy against reference\n",
    "    ẋ_diff = Diff_ẋ(dec_jac_batch, grad_fθ, ẋ_batch)\n",
    "    L_ẋ = alphas * ẋ_diff\n",
    "\n",
    "    # Compute the total loss for the entire batch\n",
    "    batchLoss = L_r + L_ż + L_ẋ\n",
    "\n",
    "    # Mean of the coefficients averaged\n",
    "    L_c = sum(abs, coeffs) / length(coeffs)\n",
    "\n",
    "    batch_loss_average = batchLoss / size(x_batch, 2) + basis_coeff * L_c\n",
    "    \n",
    "    return batch_loss_average\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set_model (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function set_model(data, num_coeff, initial_coeffs::String)\n",
    "    ld = size(data.x)[1]\n",
    "    ndim = size(data.x)[1]\n",
    "\n",
    "    encoder = Chain(\n",
    "    Dense(ndim => ld)\n",
    "    )\n",
    "\n",
    "    decoder = Chain(\n",
    "        Dense(ld => ndim)\n",
    "    )\n",
    "\n",
    "    if initial_coeffs == \"zeros\"\n",
    "        model = ( \n",
    "            (W = encoder,),\n",
    "            (W = decoder,),\n",
    "            (W = zeros(Float32, num_coeff),), \n",
    "        )\n",
    "    elseif initial_coeffs == \"ones\"\n",
    "        model = ( \n",
    "            (W = encoder,),\n",
    "            (W = decoder,),\n",
    "            (W = ones(Float32, num_coeff),),\n",
    "        )\n",
    "    else\n",
    "        error(\"initial_coeffs can be zeros or ones only\")\n",
    "    end\n",
    "\n",
    "    # set initial encoder/decoder weights to identity\n",
    "    #TODO: try not setting these to identity and see how well it works\n",
    "    model[1].W.layers[1].weight .= Matrix(LinearAlgebra.I, ndim, ld)\n",
    "    model[2].W.layers[1].weight .= Matrix(LinearAlgebra.I, ld, ndim)\n",
    "    \n",
    "    return model\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "setup_test (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function setup_test(data, method, model, alphas)\n",
    "    # Derivatives of the encoder and decoder\n",
    "    enc_jac = batched_jacobian(model[1].W, data.x)\n",
    "    dec_jac = batched_jacobian(model[2].W, model[1].W(data.x))\n",
    "\n",
    "    # encoded gradient ż = dz/dx*ẋ\n",
    "    ż_ref = enc_ż(enc_jac, data.ẋ)\n",
    "\n",
    "    # Array to store the losses\n",
    "    Initial_loss_array = Vector{Float32}()\n",
    "\n",
    "    # Store the epoch loss\n",
    "    push!(Initial_loss_array, loss(model, data.x, data.ẋ, ż_ref, dec_jac, alphas, method.basis_coeff))\n",
    "    return Initial_loss_array\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "initialModelUpdate! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function initialModelUpdate!(model_type, model, model_gradients, opt_state)\n",
    "    # Update the parameters\n",
    "    if model_type == \"fixed\"\n",
    "        Flux.Optimise.update!(opt_state[3], model[3], model_gradients[3])\n",
    "    elseif  model_type == \"symmetric\"\n",
    "        Flux.Optimise.update!(opt_state, model, model_gradients)\n",
    "        # Only q's effect q's and only p's effect p's\n",
    "        # Due to the [A, 0; 0, A] structure the effect of q on q is the same as the effect of p on p\n",
    "        temp1 = (model[1].W.layers[1].weight[1:Int(end/2), 1:Int(end/2)] + model[1].W.layers[1].weight[Int(end/2)+1:end, Int(end/2)+1:end]) / 2\n",
    "        model[1].W.layers[1].weight .= zeros(size(model[1].W.layers[1].weight))\n",
    "        model[1].W.layers[1].weight[1:Int(end/2), 1:Int(end/2)] .= temp1\n",
    "        model[1].W.layers[1].weight[Int(end/2)+1:end, Int(end/2)+1:end] .= temp1\n",
    "        model[1].W.layers[1].bias .= 0\n",
    "\n",
    "        temp2 = (model[2].W.layers[1].weight[1:Int(end/2), 1:Int(end/2)] + model[2].W.layers[1].weight[Int(end/2)+1:end, Int(end/2)+1:end]) / 2\n",
    "        model[2].W.layers[1].weight .= zeros(size(model[2].W.layers[1].weight))\n",
    "        model[2].W.layers[1].weight[1:Int(end/2), 1:Int(end/2)] .= temp2\n",
    "        model[2].W.layers[1].weight[Int(end/2)+1:end, Int(end/2)+1:end] .= temp2\n",
    "        model[2].W.layers[1].bias .= 0\n",
    "    elseif model_type == \"general\"\n",
    "        Flux.Optimise.update!(opt_state, model, model_gradients)\n",
    "    else\n",
    "        error(\"model_type must be fixed, symmetric, or general\")\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "initial_gradient_update! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function initial_gradient_update!(model, model_gradients, total_samples, num_batches, method, data, alphas, opt_state, model_type, Initial_loss_array)\n",
    "    epoch_loss = 0.0\n",
    "    # Shuffle the data indices for each epoch\n",
    "    shuffled_indices = shuffle(1:total_samples)\n",
    "    for batch in 1:num_batches\n",
    "        # Get the indices for the current batch\n",
    "        batch_start = (batch - 1) * method.batch_size + 1\n",
    "        batch_end = min(batch * method.batch_size, total_samples)\n",
    "        batch_indices = shuffled_indices[batch_start:batch_end]\n",
    "\n",
    "        # Extract the data for the current batch\n",
    "        x_batch = data.x[:, batch_indices]\n",
    "        ẋ_batch = data.ẋ[:, batch_indices]\n",
    "\n",
    "        # Derivatives of the encoder and decoder\n",
    "        enc_jac_batch = batched_jacobian(model[1].W, x_batch)\n",
    "        dec_jac_batch = batched_jacobian(model[2].W, model[1].W(x_batch))\n",
    "        \n",
    "        # Get the encoded derivative: ż\n",
    "        ż_ref_batch = enc_ż(enc_jac_batch, ẋ_batch)\n",
    "        \n",
    "        # Compute gradients using Enzyme\n",
    "        function calculateGradient!()\n",
    "            Enzyme.autodiff(Reverse, (model, x_batch, ẋ_batch, ż_ref_batch, dec_jac_batch, alphas, basis_coeff) -> loss(model, x_batch, ẋ_batch, ż_ref_batch, dec_jac_batch, alphas, basis_coeff), Active, Duplicated(model, model_gradients), Const(x_batch), Const(ẋ_batch), Const(ż_ref_batch), Const(dec_jac_batch), Const(alphas), Const(method.basis_coeff))\n",
    "        end\n",
    "        calculateGradient!()\n",
    "        \n",
    "        # Update the parameters\n",
    "        initialModelUpdate!(model_type, model, model_gradients, opt_state)\n",
    "\n",
    "        # Accumulate the loss for the current batch\n",
    "        epoch_loss += loss(model, x_batch, ẋ_batch, ż_ref_batch, dec_jac_batch, alphas, method.basis_coeff)\n",
    "    end\n",
    "    # Compute the average loss for the epoch\n",
    "    epoch_loss /= num_batches\n",
    "\n",
    "    # Store the epoch loss\n",
    "    push!(Initial_loss_array, epoch_loss)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "initial_loop (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function initial_loop(model, method, data, total_samples, num_batches, alphas, opt, model_type, Initial_loss_array)\n",
    "    opt_state = Flux.setup(opt, model)\n",
    "    for epoch in 1:1000\n",
    "        model_gradients = deepcopy(model) # Performs properly when it is inside the loop, otherwise loss increases\n",
    "        initial_gradient_update!(model, model_gradients, total_samples, num_batches, method, data, alphas, opt_state, model_type, Initial_loss_array)\n",
    "        \n",
    "        # Print loss after some iterations\n",
    "        if epoch % 200 == 0\n",
    "            println(\"Epoch $epoch: Average Loss: $(Initial_loss_array[end])\")\n",
    "            println(\"Epoch $epoch: Coefficents: $(model[3].W)\")\n",
    "            println()\n",
    "        end\n",
    "    end\n",
    "    return model, Initial_loss_array\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sparse_loss (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function sparse_loss(enc_paras, dec_paras, Ξ, model_Coeffs, x_batch, ẋ_batch, ż_ref, dec_jac_batch, alphas, basis_coeff, biginds)\n",
    "    coeffs = set_coeffs(model_Coeffs, Ξ, biginds)\n",
    "\n",
    "    enc_x_batch = enc_paras(x_batch)\n",
    "\n",
    "    # Compute the reconstruction loss for the entire batch\n",
    "    L_r = sum(abs2, dec_paras(enc_x_batch) - x_batch)\n",
    "\n",
    "    # Note: grad_fθ, dec_mult_ẋ, and L_c in loss function so model acts on terms in loss function\n",
    "    # and gradient can see that and use that for its update calculations\n",
    "\n",
    "    # encoded gradient from SINDy\n",
    "    grad_fθ = evaluate_fθ(fθ, enc_x_batch, coeffs)\n",
    "\n",
    "    # Difference b/w encoded gradients from SINDy and reference\n",
    "    L_ż = alphas / 10 * Diff_ż(grad_fθ, ż_ref)\n",
    "\n",
    "    # Difference b/w decoded-encoded gradients from SINDy against reference\n",
    "    ẋ_diff = Diff_ẋ(dec_jac_batch, grad_fθ, ẋ_batch)\n",
    "    L_ẋ = alphas * ẋ_diff\n",
    "\n",
    "    # Compute the total loss for the entire batch\n",
    "    batchLoss = L_r + L_ż + L_ẋ\n",
    "\n",
    "    # Mean of the coefficients averaged\n",
    "    L_c = sum(abs, model_Coeffs) / length(model_Coeffs)\n",
    "\n",
    "    batch_loss_average = batchLoss / size(x_batch, 2) + basis_coeff * L_c\n",
    "    \n",
    "    return batch_loss_average\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sindyModelUpdate! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function sindyModelUpdate!(model_type, model, Ξ, grad_W1, grad_W2, grad_W3, opt_state)\n",
    "    # Update the parameters\n",
    "    if model_type == \"fixed\"\n",
    "        Flux.Optimise.update!(opt_state[3], Ξ, grad_W3)\n",
    "    elseif  model_type == \"symmetric\"\n",
    "        Flux.Optimise.update!(opt_state, (model[1].W, model[2].W, Ξ), (grad_W1, grad_W2, grad_W3))\n",
    "        # Only q's effect q's and only p's effect p's\n",
    "        # Due to the [A, 0; 0, A] structure the effect of q on q is the same as the effect of p on p\n",
    "        temp1 = (model[1].W.layers[1].weight[1:Int(end/2), 1:Int(end/2)] + model[1].W.layers[1].weight[Int(end/2)+1:end, Int(end/2)+1:end]) / 2\n",
    "        model[1].W.layers[1].weight .= zeros(size(model[1].W.layers[1].weight))\n",
    "        model[1].W.layers[1].weight[1:Int(end/2), 1:Int(end/2)] .= temp1\n",
    "        model[1].W.layers[1].weight[Int(end/2)+1:end, Int(end/2)+1:end] .= temp1\n",
    "        model[1].W.layers[1].bias .= 0\n",
    "\n",
    "        temp2 = (model[2].W.layers[1].weight[1:Int(end/2), 1:Int(end/2)] + model[2].W.layers[1].weight[Int(end/2)+1:end, Int(end/2)+1:end]) / 2\n",
    "        model[2].W.layers[1].weight .= zeros(size(model[2].W.layers[1].weight))\n",
    "        model[2].W.layers[1].weight[1:Int(end/2), 1:Int(end/2)] .= temp2\n",
    "        model[2].W.layers[1].weight[Int(end/2)+1:end, Int(end/2)+1:end] .= temp2\n",
    "        model[2].W.layers[1].bias .= 0\n",
    "    elseif model_type == \"general\"\n",
    "        Flux.Optimise.update!(opt_state, (model[1].W, model[2].W, Ξ), (grad_W1, grad_W2, grad_W3))\n",
    "    else\n",
    "        error(\"model_type must be fixed, symmetric, or general\")\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sindy_gradient_update! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function copy_model(model, Ξ)\n",
    "    # Gradients of the encoder, decoder and model coefficients\n",
    "    grad_W1 = deepcopy(model[1].W)\n",
    "    grad_W2 = deepcopy(model[2].W)\n",
    "    grad_W3 = deepcopy(Ξ)\n",
    "    return grad_W1, grad_W2, grad_W3\n",
    "end\n",
    "\n",
    "function sindy_gradient_update!(model, Ξ, total_samples, num_batches, method, data, alphas, opt_state, model_type, biginds, epoch_loss_array)\n",
    "    epoch_loss = 0.0\n",
    "    # Shuffle the data indices for each epoch\n",
    "    shuffled_indices = shuffle(1:total_samples)\n",
    "\n",
    "    for batch in 1:num_batches\n",
    "        # Get the indices for the current batch\n",
    "        batch_start = (batch - 1) * method.batch_size + 1\n",
    "        batch_end = min(batch * method.batch_size, total_samples)\n",
    "        batch_indices = shuffled_indices[batch_start:batch_end]\n",
    "\n",
    "        # Extract the data for the current batch\n",
    "        x_batch = data.x[:, batch_indices]\n",
    "        ẋ_batch = data.ẋ[:, batch_indices]\n",
    "\n",
    "        # Derivatives of the encoder and decoder\n",
    "        enc_jac_batch = batched_jacobian(model[1].W, x_batch)\n",
    "        dec_jac_batch = batched_jacobian(model[2].W, model[1].W(x_batch))\n",
    "        \n",
    "        # Get the encoded derivative: ż\n",
    "        ż_ref_batch = enc_ż(enc_jac_batch, ẋ_batch)\n",
    "\n",
    "        # Gradients of the encoder, decoder and model coefficients\n",
    "        grad_W1, grad_W2, grad_W3 = copy_model(model, Ξ)\n",
    "\n",
    "        # Compute gradients using Enzyme\n",
    "        function calculateGradient2!()\n",
    "            Enzyme.autodiff(Reverse, (enc_paras, dec_paras, Ξ, model_Coeffs, x_batch, ẋ_batch, ż_ref_batch, dec_jac_batch, alphas, basis_coeff, biginds) -> sparse_loss(enc_paras, dec_paras, Ξ, model_Coeffs, x_batch, ẋ_batch, ż_ref_batch, dec_jac_batch, alphas, basis_coeff, biginds), Active, Duplicated(model[1].W, grad_W1), Duplicated(model[2].W, grad_W2), Duplicated(Ξ, grad_W3), Const(model[3].W), Const(x_batch), Const(ẋ_batch), Const(ż_ref_batch), Const(dec_jac_batch), Const(alphas), Const(method.basis_coeff), Const(biginds))\n",
    "        end\n",
    "        calculateGradient2!()\n",
    "        \n",
    "        # Update the parameters\n",
    "        sindyModelUpdate!(model_type, model, Ξ, grad_W1, grad_W2, grad_W3, opt_state)\n",
    "\n",
    "        # Accumulate the loss for the current batch\n",
    "        epoch_loss += sparse_loss(model[1].W, model[2].W, Ξ, model[3].W, x_batch, ẋ_batch, ż_ref_batch, dec_jac_batch, alphas, method.basis_coeff, biginds)\n",
    "    end\n",
    "\n",
    "    model[3].W[biginds] .= Ξ\n",
    "\n",
    "    # Compute the average loss for the epoch\n",
    "    epoch_loss /= num_batches\n",
    "    \n",
    "    # Store the epoch loss\n",
    "    push!(epoch_loss_array, epoch_loss)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sindy_loop (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function sindy_loop(model, method, data, total_samples, num_batches, alphas, opt, model_type)\n",
    "    # Array to store the losses of each SINDy loop\n",
    "    SINDy_loss_array = Vector{Vector{Float32}}()  # Store vectors of losses\n",
    "\n",
    "    # Initialize smallinds before the loop\n",
    "    smallinds = falses(size(model[3].W))\n",
    "\n",
    "    for n in 1:method.nloops\n",
    "        println(\"Iteration #$n...\")\n",
    "        println()\n",
    "\n",
    "        # find coefficients below λ threshold\n",
    "        smallinds .= abs.(model[3].W) .< method.λ\n",
    "\n",
    "        biginds = .~smallinds\n",
    "\n",
    "        # check if there are any small coefficients != 0 left\n",
    "        all(model[3].W[smallinds] .== 0) && break\n",
    "\n",
    "        # set all small coefficients to zero\n",
    "        model[3].W[smallinds] .= 0\n",
    "\n",
    "        # Set up the optimizer's state\n",
    "        Ξ = model[3].W[biginds]\n",
    "\n",
    "        # Array to store the losses of each epoch\n",
    "        epoch_loss_array = Vector{Float64}()\n",
    "\n",
    "        # Set up the optimizer's state (outside loop because it uses previous epoch information while updating)\n",
    "        opt_state = Flux.setup(opt, (model[1].W, model[2].W, Ξ))\n",
    "\n",
    "        for epoch in 1:500\n",
    "            sindy_gradient_update!(model, Ξ, total_samples, num_batches, method, data, alphas, opt_state, model_type, biginds, epoch_loss_array)\n",
    "            \n",
    "            # Print loss after some iterations\n",
    "            if epoch % 200 == 0\n",
    "                println(\"Epoch $epoch: Average Loss: $(epoch_loss_array[end])\")\n",
    "                println(\"Epoch $epoch: Coefficents: $(model[3].W)\")\n",
    "                println()\n",
    "            end\n",
    "        end\n",
    "\n",
    "        # Store the SINDy loop loss\n",
    "        push!(SINDy_loss_array, epoch_loss_array)\n",
    "        GC.gc()\n",
    "        GC.gc()\n",
    "        GC.gc()\n",
    "    end\n",
    "\n",
    "    # Convert vector of vectors to a single vector\n",
    "    SINDy_loss_array = vcat(SINDy_loss_array...)\n",
    "\n",
    "    return model, SINDy_loss_array\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "final_loop (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function final_loop(model, method, data, total_samples, num_batches, alphas, opt, model_type)\n",
    "    # Array to store the losses of each epoch\n",
    "    final_loss_array = Vector{Float64}()\n",
    "\n",
    "    # find coefficients below λ threshold\n",
    "    smallinds = abs.(model[3].W) .<= 0\n",
    "\n",
    "    biginds = .~smallinds\n",
    "\n",
    "    # Set up the optimizer's state\n",
    "    Ξ = model[3].W[biginds]\n",
    "\n",
    "    # Set up the optimizer's state\n",
    "    opt_state = Flux.setup(opt, (model[1].W, model[2].W, Ξ))\n",
    "\n",
    "    for epoch in 1:500\n",
    "        sindy_gradient_update!(model, Ξ, total_samples, num_batches, method, data, alphas, opt_state, model_type, biginds, final_loss_array)\n",
    "        \n",
    "        # Print loss after some iterations\n",
    "        if epoch % 200 == 0\n",
    "            println(\"Epoch $epoch: Average Loss: $(final_loss_array[end])\")\n",
    "            println(\"Epoch $epoch: Coefficents: $(model[3].W)\")\n",
    "            println()\n",
    "        end\n",
    "    end\n",
    "    return model, final_loss_array\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "save_plot! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function save_plot!(basis_directory, name, model_type, input_range, batch_size, loss_array, log_plot, initial_coeffs)\n",
    "    # Construct the file name based on loop variables\n",
    "    loss_plot = joinpath(basis_directory, \"$(name)_model_$(model_type)_$(input_range)_$(batch_size)_$(initial_coeffs).png\")\n",
    "    # Save the plot to the file\n",
    "    if log_plot == true\n",
    "        plot(log.(loss_array), label = \"$(name) Optimization Loss\", xlabel = \"Iterations\", ylabel = \"Log Loss\")\n",
    "    else\n",
    "        plot((loss_array), label = \"$(name) Optimization Loss\", xlabel = \"Iterations\", ylabel = \"Loss\")\n",
    "    end\n",
    "    savefig(loss_plot)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "save_model_parameters! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function save_model_parameters!(models_basis_directory, model, model_type, input_range, batch_size, initial_coeffs)\n",
    "    # Specify the file paths for the model parameters\n",
    "    model_encoder_file = joinpath(models_basis_directory, \"modelEncoders_$(model_type)_$(input_range)_$(batch_size)_$(initial_coeffs).csv\")\n",
    "    model_coeffs_file = joinpath(models_basis_directory, \"modelCoeffs_$(model_type)_$(input_range)_$(batch_size)_$(initial_coeffs).csv\")\n",
    "    model_decoder_file = joinpath(models_basis_directory, \"modelDecoders_$(model_type)_$(input_range)_$(batch_size)_$(initial_coeffs).csv\")\n",
    "\n",
    "    # Initialize the models' arrays\n",
    "    model_encoder_array = []\n",
    "    model_coeffs_array = []\n",
    "    model_decoder_array = []\n",
    "\n",
    "    push!(model_encoder_array, model[1].W.layers[1].weight)\n",
    "    push!(model_encoder_array, model[1].W.layers[1].bias)\n",
    "    # Save the encoder parameters to the file\n",
    "    writedlm(model_encoder_file, model_encoder_array, ',')\n",
    "\n",
    "    push!(model_coeffs_array, model[3].W)\n",
    "    # Save the coefficients to the file\n",
    "    writedlm(model_coeffs_file, model_coeffs_array, ',')\n",
    "\n",
    "    push!(model_decoder_array, model[2].W.layers[1].weight)\n",
    "    push!(model_decoder_array, model[2].W.layers[1].bias)\n",
    "    # Save the decoder parameters to the file\n",
    "    writedlm(model_decoder_file, model_decoder_array, ',')\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "running_loop! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function running_loop!(data, method, model, model_type, input_range, basis_directory, initial_coeffs)   \n",
    "    total_samples = size(data.x)[2]\n",
    "    num_batches = ceil(Int, total_samples / method.batch_size)\n",
    "\n",
    "    # Coefficients for the loss_kernel terms\n",
    "    alphas = round(sum(abs2, data.x) / sum(abs2, data.ẋ), sigdigits = 3)    \n",
    "    \n",
    "    Initial_loss_array = setup_test(data, method, model, alphas)\n",
    "\n",
    "    # Set up the optimizer's state\n",
    "    # TODO: could try different learning rates\n",
    "    opt = Adam(0.001, (0.9, 0.8))\n",
    "\n",
    "    log_plot = true\n",
    "    println(\"Calculating Initial Loss...\")\n",
    "    model, Initial_loss_array = initial_loop(model, method, data, total_samples, num_batches, alphas, opt, model_type, Initial_loss_array)\n",
    "    println(\"Initial Augmented Coefficients: \", model[3].W)\n",
    "    GC.gc()\n",
    "    GC.gc()\n",
    "    GC.gc()\n",
    "    println()\n",
    "\n",
    "    println(\"Calculating Sindy Loss...\")\n",
    "    model, SINDy_loss_array = sindy_loop(model, method, data, total_samples, num_batches, alphas, opt, model_type)\n",
    "    println(\"SINDy Augmented Coefficients: \", model[3].W)\n",
    "    GC.gc()\n",
    "    GC.gc()\n",
    "    GC.gc()\n",
    "    println()\n",
    "\n",
    "    # reduce learning rate before final loss calculation\n",
    "    opt = Adam(0.0001, (0.9, 0.8))\n",
    "\n",
    "    log_plot = false\n",
    "    println(\"Calculating Final Loss...\")\n",
    "    model, final_loss_array = final_loop(model, method, data, total_samples, num_batches, alphas, opt, model_type)\n",
    "    println(\"Final Augmented Coefficients: \", model[3].W)\n",
    "    GC.gc()\n",
    "    GC.gc()\n",
    "    GC.gc()\n",
    "    println()\n",
    "\n",
    "    println(\"Plotting initial loss...\")\n",
    "    save_plot!(basis_directory, \"initial\", model_type, input_range, method.batch_size, Initial_loss_array, log_plot, initial_coeffs)\n",
    "    println(\"Plotting sindy loss...\")\n",
    "    save_plot!(basis_directory, \"sindy\", model_type, input_range, method.batch_size, SINDy_loss_array, log_plot, initial_coeffs)\n",
    "    println(\"Plotting final loss...\")\n",
    "    save_plot!(basis_directory, \"final\", model_type, input_range, method.batch_size, final_loss_array, log_plot, initial_coeffs)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Running Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function main()\n",
    "    # %%\n",
    "    # --------------------\n",
    "    # Setup\n",
    "    # --------------------\n",
    "\n",
    "    println(\"Setting up...\")\n",
    "\n",
    "    # 2D system with 4 variables [q₁, q₂, p₁, p₂]\n",
    "    nd = 4\n",
    "\n",
    "    # initialize analytical function, keep λ smaller than ϵ so system is identifiable\n",
    "    ϵ = 0.5\n",
    "    m = 1\n",
    "\n",
    "    # dimension of each variable in the system\n",
    "    d = nd ÷ 2\n",
    "\n",
    "    # Symbolic variables for the states\n",
    "    z = get_z_vector(nd ÷ 2)\n",
    "\n",
    "    basis_cases = [\n",
    "    # (trig_basis_case, poly_basis_power, trig_basis_power)\n",
    "    (1, 2, 1),\n",
    "    (2, 3, 1),\n",
    "    (3, 3, 2),\n",
    "    (4, 2, 1),\n",
    "    (5, 2, 1),\n",
    "    (6, 2, 1),\n",
    "    (7, 2, 1)\n",
    "    ]\n",
    "    model_types = [\"general\", \"fixed\", \"symmetric\"]\n",
    "    sample_ranges = [5, 10, 20]\n",
    "    batch_sizes = [256, 512, 1024]\n",
    "\n",
    "    # Initialize the runtime function\n",
    "    fθ = ΔH_func_builder(d, z, poly_basis_maker(z, nd, 2), 1, 1)\n",
    "\n",
    "    # Create a directory to save the plots\n",
    "    plots_directory = \"plots\"\n",
    "    if !isdir(plots_directory)\n",
    "        mkdir(plots_directory)\n",
    "    end\n",
    "\n",
    "    # Create a directory to save the model parameters\n",
    "    models_directory = \"model_params\"\n",
    "    if !isdir(models_directory)\n",
    "        mkdir(models_directory)\n",
    "    end\n",
    "\n",
    "    # Initialize the mutable method struct\n",
    "    method = HamiltonianSINDy(z, grad_H_ana!, z, λ = 0.05, noise_level = 0.0, noiseGen_timeStep = 0.0, batch_size = 1, basis_coeff = 0.62)\n",
    "\n",
    "    for (trig_basis_case, poly_basis_power, trig_basis_power) in basis_cases\n",
    "        # Create a directory for the current basis_case plots\n",
    "        plots_basis_directory = joinpath(plots_directory, \"basis_$trig_basis_case\")\n",
    "        if !isdir(plots_basis_directory)\n",
    "            mkdir(plots_basis_directory)\n",
    "        end\n",
    "        # Create a directory for the current basis_case model parameters\n",
    "        models_basis_directory = joinpath(models_directory, \"basis_$trig_basis_case\")\n",
    "        if !isdir(models_basis_directory)\n",
    "            mkdir(models_basis_directory)\n",
    "        end\n",
    "        poly_basis = poly_basis_maker(z, nd, poly_basis_power)\n",
    "        # returns function that builds hamiltonian gradient through symbolics\n",
    "        global fθ = ΔH_func_builder(d, z, poly_basis, trig_basis_power, trig_basis_case)\n",
    "        basis, a, total_trig_args = basis_func_maker(trig_basis_case, z, d, trig_basis_power, poly_basis)\n",
    "        method.basis = basis\n",
    "        for input_range in sample_ranges\n",
    "            #TODO: could also try bigger or smaller num_samp\n",
    "            num_samp = 10 # number of samples are then actually 12*12*12*12 = 20736 for 4 variables\n",
    "            tdata = generate_training_data(num_samp, input_range, nd)\n",
    "            #TODO: could also set basis_coeff to different values and see what happens\n",
    "            # method = HamiltonianSINDy(basis, grad_H_ana!, z, λ = 0.05, noise_level = 0.0, noiseGen_timeStep = 0.0, batch_size = batch_size, basis_coeff = 0.62)\n",
    "            for batch_size in batch_sizes \n",
    "                method.batch_size = batch_size\n",
    "                for model_type in model_types\n",
    "                    #TODO: could also set initialization to \"zeros\" and see what happens\n",
    "                    initial_coeffs = \"ones\"\n",
    "                    model = set_model(tdata, length(a), initial_coeffs)\n",
    "                    println(\"basis_$trig_basis_case, model_type: $model_type, sample_domain: (-$input_range, $input_range), batch_size: $batch_size, initial_coefficients: $initial_coeffs\")\n",
    "\n",
    "                    elapsed_time = @elapsed begin\n",
    "                        running_loop!(tdata, method, model, model_type, input_range, plots_basis_directory, initial_coeffs)  \n",
    "                    end\n",
    "                    println(\"Elapsed time: $elapsed_time seconds\")\n",
    "                    save_model_parameters!(models_basis_directory, model, model_type, input_range, batch_size, initial_coeffs)\n",
    "                    println()\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "@time main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
