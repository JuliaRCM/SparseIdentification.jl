{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_threads = Threads.nthreads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Distributions\n",
    "using GeometricIntegrators\n",
    "using Optim\n",
    "using Random\n",
    "using Flux\n",
    "using Enzyme\n",
    "using Zygote\n",
    "using Distances\n",
    "using Symbolics\n",
    "using Plots\n",
    "using RuntimeGeneratedFunctions\n",
    "RuntimeGeneratedFunctions.init(@__MODULE__)\n",
    "\n",
    "gr()\n",
    "\n",
    "_prod(a, b, c, arrs...) = a .* _prod(b, c, arrs...)\n",
    "_prod(a, b) = a .* b\n",
    "_prod(a) = a\n",
    "\n",
    "# generates a vector out of symbolic arrays (p,q) with a certain dimension\n",
    "function get_z_vector(dims)\n",
    "    @variables q[1:dims]\n",
    "    @variables p[1:dims]\n",
    "    z = vcat(q,p)\n",
    "    return z\n",
    "end\n",
    "\n",
    "# make combinations of bases of just the order that is given \n",
    "# e.g order = 2 will give just the bases whose powers sum to 2\n",
    "function poly_combos(z, order, inds...)\n",
    "    if order == 0\n",
    "        return Num[1]\n",
    "    elseif order == length(inds)\n",
    "        return [_prod([z[i] for i in inds]...)]\n",
    "    else\n",
    "        start_ind = length(inds) == 0 ? 1 : inds[end]\n",
    "        return vcat([poly_combos(z, order, inds..., j) for j in start_ind:length(z)]...)\n",
    "    end\n",
    "end\n",
    "\n",
    "# gives all bases monomials up to a certain order\n",
    "function primal_monomial_basis(z, order::Int)\n",
    "    return Vector{Symbolics.Num}(vcat([poly_combos(z, i) for i in 1:order]...))\n",
    "end\n",
    "\n",
    "# calculates coefficient bases up to a certain order\n",
    "# mostly for use with trigonometric functions example sin(k*z),\n",
    "# where k is the coefficient\n",
    "function primal_coeff_basis(z, max_coeff::Int)\n",
    "    return Vector{Symbolics.Num}(vcat([k .* z for k in 1:max_coeff]...))\n",
    "end\n",
    "\n",
    "# calculates +,-,*,/ between states as a new basis\n",
    "# the return output is a set to avoid duplicates\n",
    "function primal_operator_basis(z, operator)\n",
    "    return Vector{Symbolics.Num}([operator(z[i], z[j]) for i in 1:length(z)-1 for j in i+1:length(z)] ∪ [operator(z[j], z[i]) for i in 1:length(z)-1 for j in i+1:length(z)])\n",
    "end\n",
    "\n",
    "function primal_power_basis(z, max_power::Int)\n",
    "    if max_power > 0\n",
    "        return Vector{Symbolics.Num}(vcat([z.^i for i in 1:max_power]...))\n",
    "    elseif max_power < 0\n",
    "        return Vector{Symbolics.Num}(vcat([z.^-i for i in 1:abs(max_power)]...))\n",
    "    end\n",
    "end\n",
    "\n",
    "function polynomial_basis(z::Vector{Symbolics.Num} = get_z_vector(2); polyorder::Int = 0, operator=nothing, max_coeff::Int = 0)\n",
    "    primes = primal_monomial_basis(z, polyorder)\n",
    "    primes = vcat(primes, primal_coeff_basis(z, max_coeff))\n",
    "    if operator !== nothing\n",
    "        primes = vcat(primes, primal_operator_basis(z, operator))\n",
    "    end\n",
    "    return primes\n",
    "end\n",
    "\n",
    "function trigonometric_basis(z::Vector{Symbolics.Num} = get_z_vector(2); polyorder::Int = 0, operator=nothing, max_coeff::Int = 0)\n",
    "    primes = polynomial_basis(z, polyorder = polyorder, operator = operator, max_coeff = max_coeff)\n",
    "    return vcat(sin.(primes), cos.(primes))\n",
    "end\n",
    "\n",
    "function exponential_basis(z::Vector{Symbolics.Num} = get_z_vector(2); polyorder::Int = 0, operator=nothing, max_coeff::Int = 0)\n",
    "    primes = polynomial_basis(z, polyorder = polyorder, operator = operator, max_coeff = max_coeff)\n",
    "    return exp.(primes)\n",
    "end\n",
    "\n",
    "function logarithmic_basis(z::Vector{Symbolics.Num} = get_z_vector(2); polyorder::Int = 0, operator=nothing, max_coeff::Int = 0)\n",
    "    primes = polynomial_basis(z, polyorder = polyorder, operator = operator, max_coeff = max_coeff)\n",
    "    return log.(abs.(primes))\n",
    "end\n",
    "\n",
    "function mixed_states_basis(basis::Vector{Symbolics.Num}...)\n",
    "    mixed_states = Tuple(basis)\n",
    "    \n",
    "    ham = Vector{Symbolics.Num}()\n",
    "    for i in eachindex(mixed_states)\n",
    "        for j in i+1:lastindex(mixed_states)\n",
    "            ham = vcat(ham, [mixed_states[i][k] * mixed_states[j][l] for k in 1:length(mixed_states[i]) for l in 1:length(mixed_states[j])])\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return Vector{Symbolics.Num}(ham)\n",
    "end\n",
    "\n",
    "# returns the number of required coefficients for the basis\n",
    "function get_numCoeffs(basis::Vector{Symbolics.Num})\n",
    "    return length(basis)\n",
    "end\n",
    "\n",
    "\n",
    "# gets a vector of combinations of hamiltonian basis\n",
    "function get_basis_set(basis::Vector{Symbolics.Num}...)\n",
    "    # gets a vector of combinations of basis\n",
    "    basis = vcat(basis...)\n",
    "    \n",
    "    # removes duplicates\n",
    "    basis = Vector{Symbolics.Num}(collect(unique(basis)))\n",
    "\n",
    "    return basis\n",
    "end\n",
    "\n",
    "#TODO: maybe basis shouldn't be variable number of arguments\n",
    "# returns a function that can build the gradient of the hamiltonian\n",
    "function ΔH_func_builder(d::Int, z::Vector{Symbolics.Num} = get_z_vector(d), basis::Vector{Symbolics.Num}...) \n",
    "    # nd is the total number of dimensions of all the states, e.g. if q,p each of 3 dims, that is 6 dims in total\n",
    "    nd = 2d\n",
    "    Dz = Differential.(z)\n",
    "    \n",
    "    # collects and sums combinations of basis and coefficients\"\n",
    "    basis = get_basis_set(basis...)\n",
    "   \n",
    "    # gets number of terms in the basis\n",
    "    @variables a[1:get_numCoeffs(basis)]\n",
    "    \n",
    "    # collect and sum combinations of basis and coefficients\n",
    "    ham = sum(collect(a .* basis))\n",
    "    \n",
    "    # gives derivative of the hamiltonian, but not the skew-symmetric true one\n",
    "    f = [expand_derivatives(dz(ham)) for dz in Dz]\n",
    "\n",
    "    #simplify the expression potentially to make it faster\n",
    "    f = simplify(f)\n",
    "    \n",
    "    # line below makes the vector into a hamiltonian vector field by multiplying with the skew-symmetric matrix\n",
    "    ∇H = vcat(f[d+1:2d], -f[1:d])\n",
    "    \n",
    "    # builds a function that calculates Hamiltonian gradient and converts the function to a native Julia function\n",
    "    ∇H_eval = @RuntimeGeneratedFunction(Symbolics.inject_registered_module_functions(build_function(∇H, z, a)[2]))\n",
    "    \n",
    "    return ∇H_eval\n",
    "end\n",
    "\n",
    "struct HamiltonianSINDy{T, GHT}\n",
    "    basis::Vector{Symbolics.Num} # the augmented basis for sparsification\n",
    "    analytical_fθ::GHT\n",
    "    z::Vector{Symbolics.Num} \n",
    "    λ::T # Sparsification Parameter\n",
    "    noise_level::T # Noise amplitude added to the data\n",
    "    noiseGen_timeStep::T # Time step for the integrator to get noisy data \n",
    "    nloops::Int # Sparsification Loops\n",
    "    batch_size::Int # Batch size for training\n",
    "    basis_coeff::Float64 # Coefficient for the coefficients of the basis\n",
    "    \n",
    "    function HamiltonianSINDy(basis::Vector{Symbolics.Num},\n",
    "        analytical_fθ::GHT = missing,\n",
    "        z::Vector{Symbolics.Num} = get_z_vector(2);\n",
    "        λ::T = 0.05,\n",
    "        noise_level::T = 0.00,\n",
    "        noiseGen_timeStep::T = 0.05,\n",
    "        nloops::Int = 10,\n",
    "        batch_size::Int,\n",
    "        basis_coeff::Float64) where {T, GHT <: Union{Base.Callable,Missing}}\n",
    "\n",
    "        new{T, GHT}(basis, analytical_fθ, z, λ, noise_level, noiseGen_timeStep, nloops, batch_size::Int, basis_coeff::Float64)\n",
    "    end\n",
    "end\n",
    "\n",
    "function gen_noisy_ref_data(method::HamiltonianSINDy, x)\n",
    "    # initialize timestep data for analytical solution\n",
    "    tstep = method.noiseGen_timeStep\n",
    "    tspan = (zero(tstep), tstep)\n",
    "\n",
    "    function next_timestep(x)\n",
    "        prob_ref = ODEProblem((dx, t, x, params) -> method.analytical_fθ(dx, x, params, t), tspan, tstep, x)\n",
    "        sol = integrate(prob_ref, Gauss(2))\n",
    "        sol.q[end]\n",
    "    end\n",
    "\n",
    "    data_ref = [next_timestep(_x) for _x in x]\n",
    "\n",
    "    # add noise\n",
    "    data_ref_noisy = [_x .+ method.noise_level .* randn(size(_x)) for _x in data_ref]\n",
    "\n",
    "    return data_ref_noisy\n",
    "\n",
    "end\n",
    "\n",
    "struct TrainingData{AT<:AbstractArray}\n",
    "    x::AT # initial condition\n",
    "    ẋ::AT # initial condition\n",
    "    y::AT # noisy data at next time step\n",
    "\n",
    "    TrainingData(x::AT, ẋ::AT, y::AT) where {AT} = new{AT}(x, ẋ, y)\n",
    "    TrainingData(x::AT, ẋ::AT) where {AT} = new{AT}(x, ẋ)\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# Setup\n",
    "# --------------------\n",
    "\n",
    "println(\"Setting up...\")\n",
    "\n",
    "# 2D system with 4 variables [q₁, q₂, p₁, p₂]\n",
    "const nd = 4\n",
    "\n",
    "z = get_z_vector(nd/2)\n",
    "polynomial = polynomial_basis(z, polyorder=3)\n",
    "trigonometric  = trigonometric_basis(z, max_coeff=1)\n",
    "# prime_diff = primal_operator_basis(z, -)\n",
    "# basis = get_basis_set(polynomial, trigonometric, prime_diff)\n",
    "basis = get_basis_set(polynomial, trigonometric)\n",
    "# initialize analytical function, keep λ smaller than ϵ so system is identifiable\n",
    "ϵ = 0.5\n",
    "m = 1\n",
    "\n",
    "# two-dim simple harmonic oscillator (not used anywhere only in case some testing needed)\n",
    "# H_ana(x, p, t) = ϵ * x[1]^2 + ϵ * x[2]^2 + 1/(2*m) * x[3]^2 + 1/(2*m) * x[4]^2\n",
    "# H_ana(x, p, t) = cos(x[1]) + cos(x[2]) + 1/(2*m) * x[3]^2 + 1/(2*m) * x[4]^2\n",
    "\n",
    "# Gradient function of the 2D hamiltonian\n",
    "# grad_H_ana(x) = [x[3]; x[4]; -2ϵ * x[1]; -2ϵ * x[2]]\n",
    "grad_H_ana(x) = [x[3]; x[4]; sin(x[1]); sin(x[2])]\n",
    "function grad_H_ana!(dx, x, p, t)\n",
    "    dx .= grad_H_ana(x)\n",
    "end\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Training Data\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "println(\"Generate Training Data...\")\n",
    "\n",
    "# number of samples\n",
    "num_samp = 8\n",
    "\n",
    "# samples in p and q space\n",
    "samp_range = LinRange(-10, 10, num_samp)\n",
    "\n",
    "# initialize vector of matrices to store ODE solve output\n",
    "\n",
    "# s depend on size of nd (total dims), 4 in the case here so we use samp_range x samp_range x samp_range x samp_range\n",
    "s = collect(Iterators.product(fill(samp_range, nd)...))\n",
    "\n",
    "\n",
    "# compute vector field from x state values\n",
    "x = [collect(s[i]) for i in eachindex(s)]\n",
    "\n",
    "dx = zeros(nd)\n",
    "p = 0\n",
    "t = 0\n",
    "ẋ = [grad_H_ana!(copy(dx), _x, p, t) for _x in x]\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# Compute Sparse Regression\n",
    "# ----------------------------------------\n",
    "\n",
    "# choose SINDy method\n",
    "# (λ parameter must be close to noise value so that only coeffs with value around the noise are sparsified away)\n",
    "# noiseGen_timeStep chosen randomly for now\n",
    "method = HamiltonianSINDy(basis, grad_H_ana!, z, λ = 0.05, noise_level = 0.05, noiseGen_timeStep = 0.0, batch_size = 500, basis_coeff = 0.52)\n",
    "\n",
    "# generate noisy references data at next time step\n",
    "# y = gen_noisy_ref_data(method, x)\n",
    "\n",
    "# Change to matrices for faster computations\n",
    "x = hcat(x...)\n",
    "ẋ = hcat(ẋ...)\n",
    "# y = hcat(y...)\n",
    "\n",
    "# collect training data\n",
    "tdata = TrainingData(x, ẋ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RuntimeGeneratedFunction(#=in Main=#, #=using Main=#, :((ˍ₋out, ˍ₋arg1, a)->begin\n",
       "          #= C:\\Users\\nigel\\.julia\\packages\\SymbolicUtils\\Oyu8Z\\src\\code.jl:373 =#\n",
       "          #= C:\\Users\\nigel\\.julia\\packages\\SymbolicUtils\\Oyu8Z\\src\\code.jl:374 =#\n",
       "          #= C:\\Users\\nigel\\.julia\\packages\\SymbolicUtils\\Oyu8Z\\src\\code.jl:375 =#\n",
       "          begin\n",
       "              begin\n",
       "                  #= C:\\Users\\nigel\\.julia\\packages\\Symbolics\\BQlmn\\src\\build_function.jl:519 =#\n",
       "                  #= C:\\Users\\nigel\\.julia\\packages\\SymbolicUtils\\Oyu8Z\\src\\code.jl:422 =# @inbounds begin\n",
       "                          #= C:\\Users\\nigel\\.julia\\packages\\SymbolicUtils\\Oyu8Z\\src\\code.jl:418 =#\n",
       "                          ˍ₋out[1] = (+)((+)((+)((+)((+)((+)((+)((+)((+)((+)((+)((+)((+)((+)((+)((+)((*)((^)(ˍ₋arg1[4], 2), (getindex)(a, 33)), (*)((^)(ˍ₋arg1[1], 2), (getindex)(a, 17))), (*)((^)(ˍ₋arg1[2], 2), (getindex)(a, 26))), (*)((cos)(ˍ₋arg1[3]), (getindex)(a, 37))), (*)((getindex)(a, 7), ˍ₋arg1[1])), (*)((getindex)(a, 13), ˍ₋arg1[4])), (*)((getindex)(a, 10), ˍ₋arg1[2])), (*)((*)(3, (^)(ˍ₋arg1[3], 2)), (getindex)(a, 31))), (*)((*)(-1, (sin)(ˍ₋arg1[3])), (getindex)(a, 41))), (*)((*)(2, (getindex)(a, 12)), ˍ₋arg1[3])), (*)((*)((getindex)(a, 20), ˍ₋arg1[1]), ˍ₋arg1[2])), (*)((*)((getindex)(a, 23), ˍ₋arg1[4]), ˍ₋arg1[1])), (*)((*)((getindex)(a, 29), ˍ₋arg1[4]), ˍ₋arg1[2])), (*)((*)((*)(2, (getindex)(a, 32)), ˍ₋arg1[3]), ˍ₋arg1[4])), (*)((*)((*)(2, (getindex)(a, 22)), ˍ₋arg1[3]), ˍ₋arg1[1])), (*)((*)((*)(2, (getindex)(a, 28)), ˍ₋arg1[3]), ˍ₋arg1[2])), (getindex)(a, 3))\n",
       "                          ˍ₋out[2] = (+)((+)((+)((+)((+)((+)((+)((+)((+)((+)((+)((+)((+)((+)((+)((+)((*)((^)(ˍ₋arg1[1], 2), (getindex)(a, 18)), (*)((^)(ˍ₋arg1[3], 2), (getindex)(a, 32))), (*)((^)(ˍ₋arg1[2], 2), (getindex)(a, 27))), (*)((cos)(ˍ₋arg1[4]), (getindex)(a, 38))), (*)((getindex)(a, 13), ˍ₋arg1[3])), (*)((getindex)(a, 8), ˍ₋arg1[1])), (*)((getindex)(a, 11), ˍ₋arg1[2])), (*)((*)(3, (^)(ˍ₋arg1[4], 2)), (getindex)(a, 34))), (*)((*)(-1, (sin)(ˍ₋arg1[4])), (getindex)(a, 42))), (*)((*)(2, (getindex)(a, 14)), ˍ₋arg1[4])), (*)((*)((getindex)(a, 21), ˍ₋arg1[1]), ˍ₋arg1[2])), (*)((*)((getindex)(a, 23), ˍ₋arg1[3]), ˍ₋arg1[1])), (*)((*)((getindex)(a, 29), ˍ₋arg1[3]), ˍ₋arg1[2])), (*)((*)((*)(2, (getindex)(a, 33)), ˍ₋arg1[3]), ˍ₋arg1[4])), (*)((*)((*)(2, (getindex)(a, 24)), ˍ₋arg1[4]), ˍ₋arg1[1])), (*)((*)((*)(2, (getindex)(a, 30)), ˍ₋arg1[4]), ˍ₋arg1[2])), (getindex)(a, 4))\n",
       "                          ˍ₋out[3] = (+)((+)((+)((+)((+)((+)((+)((+)((+)((+)((+)((+)((+)((+)((+)((+)((*)(-1, (getindex)(a, 1)), (*)((sin)(ˍ₋arg1[1]), (getindex)(a, 39))), (*)((*)(-1, (^)(ˍ₋arg1[3], 2)), (getindex)(a, 22))), (*)((*)(-1, (^)(ˍ₋arg1[2], 2)), (getindex)(a, 19))), (*)((*)(-3, (^)(ˍ₋arg1[1], 2)), (getindex)(a, 15))), (*)((*)(-1, (^)(ˍ₋arg1[4], 2)), (getindex)(a, 24))), (*)((*)(-1, (cos)(ˍ₋arg1[1])), (getindex)(a, 35))), (*)((*)(-1, (getindex)(a, 6)), ˍ₋arg1[2])), (*)((*)(-2, (getindex)(a, 5)), ˍ₋arg1[1])), (*)((*)(-1, (getindex)(a, 7)), ˍ₋arg1[3])), (*)((*)(-1, (getindex)(a, 8)), ˍ₋arg1[4])), (*)((*)((*)(-2, (getindex)(a, 17)), ˍ₋arg1[3]), ˍ₋arg1[1])), (*)((*)((*)(-1, (getindex)(a, 20)), ˍ₋arg1[3]), ˍ₋arg1[2])), (*)((*)((*)(-1, (getindex)(a, 23)), ˍ₋arg1[3]), ˍ₋arg1[4])), (*)((*)((*)(-1, (getindex)(a, 21)), ˍ₋arg1[4]), ˍ₋arg1[2])), (*)((*)((*)(-2, (getindex)(a, 18)), ˍ₋arg1[4]), ˍ₋arg1[1])), (*)((*)((*)(-2, (getindex)(a, 16)), ˍ₋arg1[1]), ˍ₋arg1[2]))\n",
       "                          ˍ₋out[4] = (+)((+)((+)((+)((+)((+)((+)((+)((+)((+)((+)((+)((+)((+)((+)((+)((*)(-1, (getindex)(a, 2)), (*)((sin)(ˍ₋arg1[2]), (getindex)(a, 40))), (*)((*)(-1, (^)(ˍ₋arg1[3], 2)), (getindex)(a, 28))), (*)((*)(-1, (^)(ˍ₋arg1[4], 2)), (getindex)(a, 30))), (*)((*)(-1, (^)(ˍ₋arg1[1], 2)), (getindex)(a, 16))), (*)((*)(-3, (^)(ˍ₋arg1[2], 2)), (getindex)(a, 25))), (*)((*)(-1, (cos)(ˍ₋arg1[2])), (getindex)(a, 36))), (*)((*)(-1, (getindex)(a, 6)), ˍ₋arg1[1])), (*)((*)(-1, (getindex)(a, 10)), ˍ₋arg1[3])), (*)((*)(-1, (getindex)(a, 11)), ˍ₋arg1[4])), (*)((*)(-2, (getindex)(a, 9)), ˍ₋arg1[2])), (*)((*)((*)(-1, (getindex)(a, 29)), ˍ₋arg1[3]), ˍ₋arg1[4])), (*)((*)((*)(-2, (getindex)(a, 19)), ˍ₋arg1[1]), ˍ₋arg1[2])), (*)((*)((*)(-1, (getindex)(a, 20)), ˍ₋arg1[3]), ˍ₋arg1[1])), (*)((*)((*)(-2, (getindex)(a, 26)), ˍ₋arg1[3]), ˍ₋arg1[2])), (*)((*)((*)(-1, (getindex)(a, 21)), ˍ₋arg1[4]), ˍ₋arg1[1])), (*)((*)((*)(-2, (getindex)(a, 27)), ˍ₋arg1[4]), ˍ₋arg1[2]))\n",
       "                          #= C:\\Users\\nigel\\.julia\\packages\\SymbolicUtils\\Oyu8Z\\src\\code.jl:420 =#\n",
       "                          nothing\n",
       "                      end\n",
       "              end\n",
       "          end\n",
       "      end))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# dimension of system\n",
    "d = size(tdata.x)[1] ÷ 2\n",
    "\n",
    "# returns function that builds hamiltonian gradient through symbolics\n",
    "fθ = ΔH_func_builder(d, method.z, method.basis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set_model (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function set_model(data, method)\n",
    "    #TODO: updated ld to be a parameter\n",
    "    ld = size(data.x)[1]\n",
    "    ndim = size(data.x)[1]\n",
    "\n",
    "    encoder = Chain(\n",
    "    Dense(ndim => ld, sigmoid), \n",
    "    Dense(ld => ndim)\n",
    "    )\n",
    "\n",
    "    decoder = Chain(\n",
    "    Dense(ndim => ld, sigmoid),  \n",
    "    Dense(ld => ndim)\n",
    "    )\n",
    "\n",
    "    model = ( \n",
    "        (W = encoder,),\n",
    "        (W = decoder,),\n",
    "        (W = zeros(Float32, get_numCoeffs(method.basis)), ),\n",
    "    )\n",
    "    return model\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------Enzyme Code------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function enc_ż(enc_jac_batch, ẋ_batch)\n",
    "    enc_mult_ż = zero(ẋ_batch)\n",
    "    for i in 1:size(enc_jac_batch, 2)\n",
    "        enc_mult_ż[:, i] = (enc_jac_batch[:,i,:] * (ẋ_batch[:,i]))\n",
    "    end\n",
    "    return enc_mult_ż\n",
    "end\n",
    "\n",
    "\n",
    "function evaluate_fθ(fθ, enc_x_batch, coeffs)\n",
    "    f = zero(enc_x_batch[:,1])\n",
    "    out = zero(enc_x_batch)\n",
    "    for i in 1:size(enc_x_batch, 2)\n",
    "        fθ(f, enc_x_batch[:,i], coeffs)\n",
    "        out[:,i] = f\n",
    "    end\n",
    "    return out\n",
    "end\n",
    "\n",
    "function dec_ẋ(dec_jac_batch, ż)\n",
    "    dec_mult_ẋ = zero(ż)\n",
    "    for i in 1:size(dec_jac_batch, 2)\n",
    "        dec_mult_ẋ[:, i] = dec_jac_batch[:,i,:] * ż[:,i]\n",
    "    end\n",
    "    return dec_mult_ẋ\n",
    "end\n",
    "\n",
    "function Diff_ż(grad_fθ, enc_mult_ż)\n",
    "    return sum(abs2, grad_fθ - enc_mult_ż)\n",
    "end\n",
    "\n",
    "function Diff_ẋ(dec_jac_batch, grad_fθ, ẋ_batch)\n",
    "    dec_mult_ẋ = zero(ẋ_batch)\n",
    "    for i in 1:size(dec_jac_batch, 2)\n",
    "        dec_mult_ẋ[:, i] = dec_jac_batch[:,i,:] * grad_fθ[:,i]\n",
    "    end\n",
    "    return sum(abs2, dec_mult_ẋ - ẋ_batch)\n",
    "end\n",
    "\n",
    "function loss(model, x_batch, ẋ_batch, enc_mult_ż, dec_jac_batch, alphas, method)\n",
    "    enc_x_batch = model[1].W(x_batch)\n",
    "\n",
    "    coeffs = model[3].W\n",
    "\n",
    "    # Compute the reconstruction loss for the entire batch\n",
    "    L_r = sum(abs2, model[2].W(enc_x_batch) - x_batch)\n",
    "\n",
    "    # Note: grad_fθ, dec_mult_ẋ, and L_c in loss function so model acts on terms in loss function\n",
    "    # and gradient can see that and use that for its update calculations\n",
    "\n",
    "    # encoded gradient from SINDy\n",
    "    grad_fθ = evaluate_fθ(fθ, enc_x_batch, coeffs)\n",
    "\n",
    "    # Difference b/w encoded gradients from SINDy and reference\n",
    "    L_ż = alphas / 10 * Diff_ż(grad_fθ, enc_mult_ż)\n",
    "\n",
    "    # decoded SINDy gradient ẋ = dx/dz*grad_fθ\n",
    "    # dec_mult_ẋ = dec_ẋ(dec_jac_batch, grad_fθ)\n",
    "\n",
    "    # Difference b/w decoded-encoded gradients from SINDy against reference\n",
    "    ẋ_diff = Diff_ẋ(dec_jac_batch, grad_fθ, ẋ_batch)\n",
    "    L_ẋ = alphas * ẋ_diff\n",
    "\n",
    "    # Compute the total loss for the entire batch\n",
    "    batchLoss = L_r + L_ż + L_ẋ\n",
    "\n",
    "    # Mean of the coefficients averaged\n",
    "    L_c = sum(abs, coeffs) / length(coeffs)\n",
    "\n",
    "    batch_loss_average = batchLoss / size(x_batch, 2) + method.basis_coeff * L_c\n",
    "    \n",
    "    return batch_loss_average\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define initial model\n",
    "model = set_model(tdata, method)\n",
    "\n",
    "# dmodel = Flux.fmap(model) do x\n",
    "#     x isa Array ? zero(x) : x\n",
    "# end\n",
    "\n",
    "model_gradients = deepcopy(model)\n",
    "\n",
    "# Flux gradient has problem working with the structure data directly\n",
    "x = Float32.(tdata.x)\n",
    "ẋ = Float32.(tdata.ẋ)\n",
    "\n",
    "total_samples = size(x)[2]\n",
    "num_batches = ceil(Int, total_samples / method.batch_size)\n",
    "\n",
    "# Coefficients for the loss_kernel terms\n",
    "alphas = round(sum(abs2, x) / sum(abs2, ẋ), sigdigits = 3)\n",
    "\n",
    "# Derivatives of the encoder and decoder\n",
    "enc_jac_batch = batched_jacobian(model[1].W, x)\n",
    "dec_jac_batch = batched_jacobian(model[2].W, model[1].W(x))\n",
    "\n",
    "# encoded gradient ż = ẋ*dz/dx\n",
    "enc_mult_ż = enc_ż(enc_jac_batch, ẋ)\n",
    "\n",
    "initial_loss_array = Vector{Float32}()\n",
    "\n",
    "# initial guess\n",
    "println(\"Initial Guess...\")\n",
    "\n",
    "# Set up the optimizer's state\n",
    "opt_state = Flux.setup(Adam(), model)\n",
    "\n",
    "println(\"Coeffs: $(model[3].W)\")\n",
    "println()\n",
    "println(\"initial total loss:\", loss(model, x, ẋ, enc_mult_ż, dec_jac_batch, alphas, method))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Array to store the losses\n",
    "epoch_loss_array = Vector{Float32}()\n",
    "\n",
    "for epoch in 1:500\n",
    "    epoch_loss = 0.0\n",
    "    # Shuffle the data indices for each epoch\n",
    "    shuffled_indices = shuffle(1:total_samples)\n",
    "    for batch in 1:num_batches\n",
    "        # Get the indices for the current batch\n",
    "        batch_start = (batch - 1) * method.batch_size + 1\n",
    "        batch_end = min(batch * method.batch_size, total_samples)\n",
    "        batch_indices = shuffled_indices[batch_start:batch_end]\n",
    "\n",
    "        # Extract the data for the current batch\n",
    "        x_batch = x[:, batch_indices]\n",
    "        ẋ_batch = ẋ[:, batch_indices]\n",
    "        # Derivatives of the encoder and decoder\n",
    "        enc_jac_batch = batched_jacobian(model[1].W, x_batch)\n",
    "        dec_jac_batch = batched_jacobian(model[2].W, model[1].W(x_batch))\n",
    "        # encoded gradient ż = ẋ*dz/dx\n",
    "        enc_mult_ż = enc_ż(enc_jac_batch, ẋ_batch)\n",
    "\n",
    "        # Compute gradients using Enzyme\n",
    "        Enzyme.autodiff(Reverse, (model, x_batch, ẋ_batch, enc_mult_ż, dec_jac_batch, alphas, method) -> loss(model, x_batch, ẋ_batch, enc_mult_ż, dec_jac_batch, alphas, method), Active, Duplicated(model, model_gradients), Const(x_batch), Const(ẋ_batch), Const(enc_mult_ż), Const(dec_jac_batch), Const(alphas), Const(method))\n",
    "\n",
    "        # Update the parameters\n",
    "        Flux.Optimise.update!(opt_state, model, model_gradients)\n",
    "\n",
    "        # Accumulate the loss for the current batch\n",
    "        epoch_loss += loss(model, x_batch, ẋ_batch, enc_mult_ż, dec_jac_batch, alphas, method)\n",
    "    end\n",
    "    # Compute the average loss for the epoch\n",
    "    epoch_loss /= num_batches\n",
    "\n",
    "    # Store the epoch loss\n",
    "    push!(epoch_loss_array, epoch_loss)\n",
    "\n",
    "    # Print loss after some iterations\n",
    "    if epoch % 100 == 0\n",
    "        println(\"Epoch $epoch: Average Loss: $epoch_loss\")\n",
    "        println(\"Epoch $epoch: Coefficents: $(model[3].W)\")\n",
    "        println()\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------Zygote Code------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evaluate_fθ (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function evaluate_fθ(x_batch, model, fθ)\n",
    "    out = Zygote.Buffer(x_batch)\n",
    "    f = Zygote.Buffer(x_batch[:,1])\n",
    "    for i in 1:size(x_batch, 2)\n",
    "        fθ(f, model[1].W(x_batch)[:,i], model[3].W)\n",
    "        out[:,i] = f[:]\n",
    "    end\n",
    "    return copy(out) \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "batched_jacobian (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Needed because Flux.gradient can't handle Flux.jacobian\n",
    "function batched_jacobian(model_layer, x_batch)\n",
    "    output_dim = size(model_layer(x_batch[:, 1]))[1]\n",
    "    batch_size = size(x_batch, 2)\n",
    "    \n",
    "    batch_jac = zeros(Float32, output_dim, batch_size, size(x_batch, 1))\n",
    "    \n",
    "    for i in 1:batch_size\n",
    "        x_input = x_batch[:, i]\n",
    "        jac = Flux.jacobian(model_layer, x_input)[1]\n",
    "        batch_jac[:, i, :] = jac\n",
    "    end\n",
    "    return batch_jac\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dec_ẋ (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get ż from encoder derivative and ẋ\n",
    "function enc_ż(enc_jac_batch, ẋ_batch)\n",
    "    enc_mult_ż = Zygote.Buffer(ẋ_batch)\n",
    "    for i in 1:size(enc_jac_batch, 2)\n",
    "        enc_mult_ż[:, i] = (enc_jac_batch[:,i,:] * (ẋ_batch[:,i]))\n",
    "    end\n",
    "    return copy(enc_mult_ż)\n",
    "end\n",
    "\n",
    "# Get ẋ from decoder derivative and ż\n",
    "function dec_ẋ(dec_jac_batch, ż)\n",
    "    dec_mult_ẋ = Zygote.Buffer(ż)\n",
    "    for i in 1:size(dec_jac_batch, 2)\n",
    "        dec_mult_ẋ[:, i] = dec_jac_batch[:,i,:] * ż[:,i]\n",
    "    end\n",
    "    return copy(dec_mult_ẋ)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function loss(model, x_batch, ẋ_batch, enc_jac_batch, dec_jac_batch, enc_mult_ż, method, fθ, alphas)\n",
    "    # Compute the reconstruction loss for the entire batch\n",
    "    L_r = sum(abs2, model[2].W(model[1].W(x_batch)) - x_batch)\n",
    "\n",
    "    ### Note: grad_fθ, dec_mult_ẋ, and L_c in loss function so model acts on terms in loss function\n",
    "    ### and gradient can see that and use that for its update calculations\n",
    "\n",
    "    # encoded gradient from SINDy\n",
    "    grad_fθ = evaluate_fθ(x_batch, model, fθ)\n",
    "\n",
    "    L_ż = alphas / 10 * sum(abs2, enc_mult_ż - grad_fθ)\n",
    "\n",
    "    # decoded SINDy gradient ẋ = dx/dz*grad_fθ\n",
    "    dec_mult_ẋ = dec_ẋ(dec_jac_batch, grad_fθ)\n",
    "\n",
    "    L_ẋ = alphas * sum(abs2, dec_mult_ẋ  - ẋ_batch)\n",
    "\n",
    "    # Compute the total loss for the entire batch\n",
    "    batchLoss = L_r + L_ż + L_ẋ\n",
    "\n",
    "    # Mean of the coefficients averaged\n",
    "    coeffs = model[3].W\n",
    "    L_c = sum(abs, coeffs) / length(coeffs)\n",
    "\n",
    "    batch_loss_average = batchLoss / size(x_batch, 2) + method.basis_coeff * L_c\n",
    "    \n",
    "    return batch_loss_average\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((W = Chain(Dense(4 => 4, σ), Dense(4 => 4)),), (W = Chain(Dense(4 => 4, σ), Dense(4 => 4)),), (W = Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = set_model(tdata, method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×4096 Matrix{Float32}:\n",
       " -0.104432   -0.154544  -0.290448  …  0.289954  0.154615  0.104467\n",
       " -0.269342   -0.306433  -1.42981      1.43657   0.307815  0.269566\n",
       " -0.0365111  -0.122082  -0.616071     0.617489  0.122315  0.0365417\n",
       " -0.309275   -0.371181  -1.12722      1.13055   0.372116  0.309462"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Flux gradient has problem working with the structure data directly\n",
    "x = Float32.(tdata.x)\n",
    "ẋ = Float32.(tdata.ẋ)\n",
    "\n",
    "total_samples = size(x)[2]\n",
    "num_batches = ceil(Int, total_samples / method.batch_size)\n",
    "\n",
    "# Coefficients for the loss_kernel terms\n",
    "alphas = round(sum(abs2, x) / sum(abs2, ẋ), sigdigits = 3)\n",
    "\n",
    "# Derivatives of the encoder and decoder\n",
    "enc_jac_batch = batched_jacobian(model[1].W, x)\n",
    "dec_jac_batch = batched_jacobian(model[2].W, model[1].W(x))\n",
    "\n",
    "# encoded gradient ż = ẋ*dz/dx\n",
    "enc_mult_ż = enc_ż(enc_jac_batch, ẋ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "342.00853027578444"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "initial_loss_array = Vector{Float32}()\n",
    "\n",
    "# initial guess\n",
    "println(\"Initial Guess...\")\n",
    "\n",
    "# Set up the optimizer's state\n",
    "opt_state = Flux.setup(Adam(), model)\n",
    "\n",
    "@show(\"Coeffs: $(model[3].W)\")\n",
    "@show(\"initial total loss:\", loss(model, x, ẋ, enc_jac_batch, dec_jac_batch, enc_mult_ż, method, fθ, alphas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array to store the losses\n",
    "epoch_loss_array = Vector{Float32}()\n",
    "\n",
    "for epoch in 1:2\n",
    "    epoch_loss = 0.0\n",
    "    # Shuffle the data indices for each epoch\n",
    "    shuffled_indices = shuffle(1:total_samples)\n",
    "    \n",
    "    for batch in 1:num_batches\n",
    "        # Get the indices for the current batch\n",
    "        batch_start = (batch - 1) * method.batch_size + 1\n",
    "        batch_end = min(batch * method.batch_size, total_samples)\n",
    "        batch_indices = shuffled_indices[batch_start:batch_end]\n",
    "\n",
    "        # Extract the data for the current batch\n",
    "        x_batch = x[:, batch_indices]\n",
    "        ẋ_batch = ẋ[:, batch_indices]\n",
    "        # Derivatives of the encoder and decoder\n",
    "        enc_jac_batch = batched_jacobian(model[1].W, x_batch)\n",
    "        dec_jac_batch = batched_jacobian(model[2].W, model[1].W(x_batch))\n",
    "        # encoded gradient ż = ẋ*dz/dx\n",
    "        enc_mult_ż = enc_ż(enc_jac_batch, ẋ_batch)\n",
    "\n",
    "        # Compute gradients using Flux\n",
    "        gradients = Flux.gradient(model -> loss(model, x_batch, ẋ_batch, enc_jac_batch, dec_jac_batch, enc_mult_ż, method, fθ, alphas), model)[1]\n",
    "\n",
    "        # Update the parameters\n",
    "        Flux.Optimise.update!(opt_state, model, gradients)\n",
    "\n",
    "        # Accumulate the loss for the current batch\n",
    "        epoch_loss += loss(model, x_batch, ẋ_batch, enc_jac_batch, dec_jac_batch, enc_mult_ż, method, fθ, alphas)\n",
    "    end\n",
    "    # Compute the average loss for the epoch\n",
    "    epoch_loss /= num_batches\n",
    "\n",
    "    # Store the epoch loss\n",
    "    push!(epoch_loss_array, epoch_loss)\n",
    "\n",
    "    # Print loss after some iterations\n",
    "    if epoch % 20 == 0\n",
    "        println(\"Epoch $epoch: Average Loss: $epoch_loss\")\n",
    "        println(\"Epoch $epoch: Coefficents: $(model[3].W)\")\n",
    "        println()\n",
    "    end\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.3",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
