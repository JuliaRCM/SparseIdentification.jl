{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ForwardDiff\n",
    "using Symbolics\n",
    "using RuntimeGeneratedFunctions\n",
    "using Distributions\n",
    "using GeometricIntegrators\n",
    "using Optim\n",
    "using Plots\n",
    "using Random\n",
    "using Distances\n",
    "RuntimeGeneratedFunctions.init(@__MODULE__)\n",
    "gr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const DEFAULT_LAMBDA = 0.05\n",
    "const DEFAULT_NOISE_LEVEL = 0.05\n",
    "const DEFAULT_NLOOPS = 10\n",
    "const DEFAULT_t₂_data_timeStep = 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct HamiltonianSINDy{T, GHT}\n",
    "    # basis::Vector{Symbolics.Num} # the augmented basis for sparsification\n",
    "    analytical_fθ::GHT\n",
    "    z::Vector{Symbolics.Num} \n",
    "    λ::T # Sparsification Parameter\n",
    "    noise_level::T # Noise amplitude added to the data\n",
    "    t₂_data_timeStep::T # Time step for the integrator to get noisy data \n",
    "    nloops::Int # Sparsification Loops\n",
    "    \n",
    "    function HamiltonianSINDy(\n",
    "        analytical_fθ::GHT = missing,\n",
    "        z::Vector{Symbolics.Num} = get_z_vector(2);\n",
    "        λ::T = DEFAULT_LAMBDA,\n",
    "        noise_level::T = DEFAULT_NOISE_LEVEL,\n",
    "        t₂_data_timeStep::T = DEFAULT_t₂_data_timeStep,\n",
    "        nloops = DEFAULT_NLOOPS) where {T, GHT <: Union{Base.Callable,Missing}}\n",
    "\n",
    "        new{T, GHT}(analytical_fθ, z, λ, noise_level, t₂_data_timeStep, nloops)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct TrainingData{AT<:AbstractArray}\n",
    "    x::AT # initial condition\n",
    "    ẋ::AT # initial condition\n",
    "    y::AT # noisy data at next time step\n",
    "\n",
    "    TrainingData(x::AT, ẋ::AT, y::AT) where {AT} = new{AT}(x, ẋ, y)\n",
    "    TrainingData(x::AT, ẋ::AT) where {AT} = new{AT}(x, ẋ)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract type VectorField end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The _prod function takes one or more input arrays and performs an element-wise multiplication on them.\n",
    "_prod(a, b, c, arrs...) = a .* _prod(b, c, arrs...)\n",
    "_prod(a, b) = a .* b\n",
    "_prod(a) = a\n",
    "\n",
    "# generates a vector out of symbolic arrays (p,q) with a certain dimension\n",
    "function get_z_vector(dims)\n",
    "    @variables q[1:dims]\n",
    "    @variables p[1:dims]\n",
    "    z = vcat(q,p)\n",
    "    return z\n",
    "end\n",
    "\n",
    "\n",
    "# returns the number of required coefficients for the basis\n",
    "function get_numCoeffs(basis::Vector{Symbolics.Num})\n",
    "    return length(basis)\n",
    "end\n",
    "\n",
    "\n",
    "# gets a vector of combinations of hamiltonian basis\n",
    "function get_basis_set(basis::Vector{Symbolics.Num}...)\n",
    "    # gets a vector of combinations of basis\n",
    "    basis = vcat(basis...)\n",
    "    \n",
    "    # removes duplicates\n",
    "    basis = Vector{Symbolics.Num}(collect(unique(basis)))\n",
    "\n",
    "    return basis\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct HamiltonianSINDyVectorField{DT,CT,GHT} <: VectorField\n",
    "    # basis::BT\n",
    "    coefficients::CT\n",
    "    fθ::GHT\n",
    "\n",
    "    function HamiltonianSINDyVectorField(coefficients::CT, fθ::GHT) where {DT, CT <: AbstractVector{DT}, GHT <: Base.Callable}\n",
    "        new{DT,CT,GHT}(coefficients, fθ)\n",
    "    end\n",
    "end\n",
    "\n",
    "\n",
    "function VectorField(method::HamiltonianSINDy, data::TrainingData; solver = BFGS())\n",
    "    # Check if the first dimension of x is even\n",
    "    size(data.x[begin], 1) % 2 == 0 || throw(ArgumentError(\"The first dimension of x must be even.\"))\n",
    "\n",
    "    # dimension of system\n",
    "    d = size(data.x[begin], 1) ÷ 2\n",
    "    fθ = ΔH_func_builder_two(d, method.z)\n",
    "    coeffs = sparsify_extra(method, fθ, data.x, data.ẋ, solver)    \n",
    "    HamiltonianSINDyVectorField(coeffs, fθ)\n",
    "end\n",
    "\n",
    "\n",
    "\" wrapper function for generalized SINDY hamiltonian gradient.\n",
    "Needs the output of fθ to work! \"\n",
    "function (vectorfield::HamiltonianSINDyVectorField)(dz, z)\n",
    "    vectorfield.fθ(dz, z, vectorfield.coefficients)\n",
    "    return dz\n",
    "end\n",
    "\n",
    "(vectorfield::HamiltonianSINDyVectorField)(dz, z, p, t) = vectorfield(dz, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "function ΔH_func_builder_two(d::Int, z::Vector{Symbolics.Num} = get_z_vector(d)) \n",
    "    # nd is the total number of dimensions of all the states, e.g. if q,p each of 3 dims, that is 6 dims in total\n",
    "    # nd = 2d\n",
    "    \n",
    "    Dz = Differential.(z)\n",
    "   \n",
    "    # gets number of terms in the basis\n",
    "    @variables a[1:4]\n",
    "\n",
    "    coeffsMat = Matrix{Symbolics.Num}([a[1] a[2]; a[3] a[4]])\n",
    "    qs = cos.(coeffsMat * z[1:2])\n",
    "    ps= 0.5*(transpose(coeffsMat*z[3:4])*(coeffsMat*z[3:4]))\n",
    "    basis = vcat(qs,ps)\n",
    "    basis = Vector{Symbolics.Num}(basis)\n",
    "\n",
    "    # collect and sum combinations of basis and coefficients\n",
    "    ham = sum(collect(basis))\n",
    "\n",
    "    # gives derivative of the hamiltonian, but not the skew-symmetric true one\n",
    "    f = [expand_derivatives(dz(ham)) for dz in Dz]\n",
    "\n",
    "    #simplify the expression potentially to make it faster\n",
    "    f = simplify(f)\n",
    "    \n",
    "    # line below makes the vector into a hamiltonian vector field by multiplying with the skew-symmetric matrix\n",
    "    ΔH = vcat(f[d+1:2d], -f[1:d])\n",
    "    \n",
    "    # builds a function that calculates Hamiltonian gradient and converts the function to a native Julia function\n",
    "    ΔH_eval = @RuntimeGeneratedFunction(Symbolics.inject_registered_module_functions(build_function(ΔH, z, a)[2]))\n",
    "    \n",
    "    return ΔH_eval\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function sparsify_extra(method::HamiltonianSINDy, fθ, x, ẋ, solver)\n",
    "    # add noise\n",
    "    ẋnoisy = [_ẋ .+ method.noise_level .* randn(size(_ẋ)) for _ẋ in ẋ]\n",
    "\n",
    "    coeffs = randn(4)\n",
    "    println(\"Initial coeffs:\", coeffs)\n",
    "    \n",
    "    # define loss function\n",
    "    function loss_kernel(x₀, x̃, fθ, a)\n",
    "        # gradient of SINDy Hamiltonian problem\n",
    "        f = zeros(eltype(a), axes(x₀))\n",
    "        \n",
    "        # gradient at current (x) values\n",
    "        fθ(f, x₀, a)\n",
    "\n",
    "        # calculate square euclidean distance\n",
    "        sqeuclidean(f,x̃)\n",
    "    end\n",
    "\n",
    "    # define loss function\n",
    "    function loss(a::AbstractVector)\n",
    "        mapreduce(z -> loss_kernel(z..., fθ, a), +, zip(x, ẋnoisy))\n",
    "    end\n",
    "    \n",
    "    # initial guess\n",
    "    println(\"Initial Guess...\")\n",
    "    result = Optim.optimize(loss, coeffs, solver, Optim.Options(show_trace=true); autodiff = :forward)\n",
    "    coeffs .= result.minimizer\n",
    "\n",
    "    println(result)\n",
    "    println(coeffs)\n",
    "\n",
    "    for n in 1:method.nloops\n",
    "        println(\"Iteration #$n...\")\n",
    "\n",
    "        # find coefficients below λ threshold\n",
    "        smallinds = abs.(coeffs) .< method.λ\n",
    "        biginds = .~smallinds\n",
    "\n",
    "        # check if there are any small coefficients != 0 left\n",
    "        all(coeffs[smallinds] .== 0) && break\n",
    "\n",
    "        # set all small coefficients to zero\n",
    "        coeffs[smallinds] .= 0\n",
    "\n",
    "        # Regress dynamics onto remaining terms to find sparse coeffs\n",
    "        function sparseloss(b::AbstractVector)\n",
    "            c = zeros(eltype(b), axes(coeffs))\n",
    "            c[biginds] .= b\n",
    "            loss(c)\n",
    "        end\n",
    "\n",
    "        # b is a reference to coeffs[biginds]\n",
    "        b = coeffs[biginds]\n",
    "        result = Optim.optimize(sparseloss, b, solver, Optim.Options(show_trace=true); autodiff = :forward)\n",
    "        b .= result.minimizer\n",
    "\n",
    "        println(result)\n",
    "        println(coeffs)\n",
    "    end\n",
    "\n",
    "    return coeffs\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# Setup\n",
    "# --------------------\n",
    "\n",
    "println(\"Setting up...\")\n",
    "\n",
    "# 2D system with 4 variables [q₁, q₂, p₁, p₂]\n",
    "const nd = 4\n",
    "d = nd ÷ 2\n",
    "z = get_z_vector(Int(nd/2))\n",
    "\n",
    "# Gradient function of the 2D hamiltonian\n",
    "grad_H_ana(x) = [x[3]; x[4]; sin(x[1]); sin(x[2])]\n",
    "function grad_H_ana!(dx, x, p, t)\n",
    "    dx .= grad_H_ana(x)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"Generate Training Data...\")\n",
    "\n",
    "# number of samples\n",
    "num_samp = 25\n",
    "\n",
    "# samples in p and q space\n",
    "samp_range = LinRange(-1, 1, num_samp)\n",
    "\n",
    "# s depend on size of nd (total dims), 4 in the case here so we use samp_range x samp_range x samp_range x samp_range\n",
    "s = collect(Iterators.product(fill(samp_range, nd)...))\n",
    "\n",
    "# compute vector field from x state values\n",
    "x = [collect(s[i]) for i in eachindex(s)]\n",
    "\n",
    "dx = zeros(nd)\n",
    "p = 0\n",
    "t = 0\n",
    "ẋ = [grad_H_ana!(copy(dx), _x, p, t) for _x in x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = [0.5 0.5; 0.5 -0.5]\n",
    "\n",
    "for i in 1:size(x,1)\n",
    "    x[i][1:2] .= W * x[i][1:2]\n",
    "    x[i][3:4] .= W * x[i][3:4] \n",
    "    ẋ[i][1:2] .= W * ẋ[i][1:2]\n",
    "    ẋ[i][3:4] .= W * ẋ[i][3:4]  \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** t₂_data_timeStep chosen randomly\n",
    "method = HamiltonianSINDy(grad_H_ana!, z, λ = 0.05, noise_level = 0.0)\n",
    "\n",
    "# collect training data\n",
    "tdata = TrainingData(x, ẋ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorfield = VectorField(method, tdata, solver=BFGS())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare gradients of transformed data and SINDy prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ẋ[2]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0\n",
    "t = 0\n",
    "dx = zero(x[2])\n",
    "vectorfield(dx, x[2], p, t)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare inverse of W to SINDy predicted coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transpose reshaped vectorfield.coefficients to get the row major shape for \\[\\begin{matrix} a[1] & a[2] \\\\ a[3] & a[4] \\end{matrix}\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transpose(reshape(vectorfield.coefficients, 2, 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Code for decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function sparsify_extra_two(method::HamiltonianSINDy, fθ, x, ẋ, solver, basisType = \"B1\")\n",
    "    # add noise\n",
    "    ẋnoisy = [_ẋ .+ method.noise_level .* randn(size(_ẋ)) for _ẋ in ẋ]\n",
    "\n",
    "    # coeffs initialized to a vector of zeros b/c sparsification is easier if some values already start small\n",
    "    if basisType == \"B1\"\n",
    "        coeffs = [rand(Float64, 8); zeros(get_numCoeffs(method.basis))]\n",
    "    elseif basisType == \"B6\"\n",
    "        coeffs = [rand(Float64, 8); zeros(get_numCoeffs(method.basis))]\n",
    "    end\n",
    "    \n",
    "    # define loss function\n",
    "    function loss_kernel(x₀, x̃, fθ, a)\n",
    "        # gradient of SINDy Hamiltonian problem\n",
    "        f = zeros(eltype(a), axes(x₀))\n",
    "        W_one = reshape(a[1:4], 2, 2)\n",
    "\n",
    "        W_two = reshape(a[5:8], 2, 2)\n",
    "\n",
    "        z_state = zeros(eltype(a), axes(x₀))\n",
    "        z_state[1:2] = W_one * ForwardDiff.value(x₀)[1:2]\n",
    "        z_state[3:4] = W_one * ForwardDiff.value(x₀)[3:4]\n",
    "        # x_state[1:2] = W * x₀[1:2]\n",
    "        # x_state[3:4] = W * x₀[3:4]\n",
    "\n",
    "        z_state = ForwardDiff.value(z_state)[1:4]\n",
    "\n",
    "        # gradient at current (x) values\n",
    "        fθ(f, z_state, a[9:end])\n",
    "\n",
    "        # # decode gradient\n",
    "        # f[1:2] = W * f[1:2]\n",
    "        # f[3:4] = W * f[3:4]\n",
    "\n",
    "        # x_decode = zeros(eltype(a), axes(x₀))\n",
    "        # x_decode[1:2] = W * x̃[1:2]\n",
    "        # x_decode[3:4] = W * x̃[3:4]\n",
    "\n",
    "        z_grad = zeros(eltype(a), axes(x̃))\n",
    "        z_grad[1:2] = W_one * ForwardDiff.value(x̃)[1:2]\n",
    "        z_grad[3:4] = W_one * ForwardDiff.value(x̃)[3:4]\n",
    "\n",
    "        z_grad = ForwardDiff.value(z_grad)[1:4]\n",
    "        # calculate square euclidean distance for ż error\n",
    "        z_diff = sqeuclidean(f, z_grad)\n",
    "\n",
    "        x_grad = zeros(eltype(f), axes(f))\n",
    "        x_grad[1:2] = W_two * f[1:2]\n",
    "        x_grad[3:4] = W_two * f[3:4]\n",
    "        # calculate square euclidean distance ẋ error\n",
    "        x_diff = sqeuclidean(x_grad, x̃)\n",
    "\n",
    "        x_state = zeros(eltype(z_state), axes(z_state))\n",
    "        x_state[1:2] = W_two * z_state[1:2]\n",
    "        x_state[3:4] = W_two * z_state[3:4]\n",
    "\n",
    "        x_rec = sqeuclidean(x_state, x₀)\n",
    "\n",
    "        alphas = round(sum(abs2, x₀) / sum(abs2, x̃), sigdigits = 3)\n",
    "        return alphas/10 * z_diff + x_diff + x_rec\n",
    "    end\n",
    "\n",
    "    # define loss function\n",
    "    function loss(a::AbstractVector)\n",
    "        mapreduce(z -> loss_kernel(z..., fθ, a), +, zip(x, ẋnoisy))\n",
    "    end\n",
    "    \n",
    "    # initial guess\n",
    "    println(\"Initial Guess...\")\n",
    "    result = Optim.optimize(loss, coeffs, solver, Optim.Options(show_trace=true, iterations=200); autodiff = :forward)\n",
    "    coeffs .= result.minimizer\n",
    "\n",
    "    println(result)\n",
    "    println(coeffs)\n",
    "\n",
    "    for n in 1:method.nloops\n",
    "        println(\"Iteration #$n...\")\n",
    "        grad_coeffs = coeffs[9:end]\n",
    "        # find coefficients below λ threshold\n",
    "        smallinds = abs.(grad_coeffs) .< method.λ\n",
    "        biginds = .~smallinds\n",
    "\n",
    "        # check if there are any small coefficients != 0 left\n",
    "        all(grad_coeffs[smallinds] .== 0) && break\n",
    "\n",
    "        # set all small coefficients to zero\n",
    "        grad_coeffs[smallinds] .= 0\n",
    "\n",
    "        # Regress dynamics onto remaining terms to find sparse grad_coeffs\n",
    "        function sparseloss(b::AbstractVector, coeffs, biginds)\n",
    "            c = zeros(eltype(b), axes(coeffs))\n",
    "            d = zeros(eltype(b), axes(coeffs[9:end]))\n",
    "            c[1:8] .= b[1:8]\n",
    "            d[biginds] .= b[9:end]\n",
    "            c[9:end] .= d\n",
    "            loss(c)\n",
    "        end\n",
    "\n",
    "        # b is a reference to the encoder-decoder weights and grad_coeffs[biginds]\n",
    "        b = [coeffs[1:8]; grad_coeffs[biginds]]\n",
    "        result = Optim.optimize(b -> sparseloss(b, coeffs, biginds), b, solver, Optim.Options(show_trace=true, iterations=200); autodiff = :forward)\n",
    "        b .= result.minimizer\n",
    "\n",
    "        coeffs[1:8] .= b[1:8]\n",
    "        grad_coeffs[biginds] = b[9:end]\n",
    "        \n",
    "        println(result)\n",
    "        println(coeffs)\n",
    "    end\n",
    "\n",
    "    return coeffs\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
