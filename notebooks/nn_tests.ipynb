{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check number of threads being used first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_threads = Threads.nthreads()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code from Sparsification module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Distributions\n",
    "using GeometricIntegrators\n",
    "using Optim\n",
    "using Random\n",
    "using Flux\n",
    "using Distances\n",
    "using Symbolics\n",
    "using ForwardDiff\n",
    "using Plots\n",
    "using LinearAlgebra\n",
    "using RuntimeGeneratedFunctions\n",
    "RuntimeGeneratedFunctions.init(@__MODULE__)\n",
    "\n",
    "gr()\n",
    "\n",
    "_prod(a, b, c, arrs...) = a .* _prod(b, c, arrs...)\n",
    "_prod(a, b) = a .* b\n",
    "_prod(a) = a\n",
    "\n",
    "# generates a vector out of symbolic arrays (p,q) with a certain dimension\n",
    "function get_z_vector(dims)\n",
    "    @variables q[1:dims]\n",
    "    @variables p[1:dims]\n",
    "    z = vcat(q,p)\n",
    "    return z\n",
    "end\n",
    "\n",
    "# make combinations of bases of just the order that is given \n",
    "# e.g order = 2 will give just the bases whose powers sum to 2\n",
    "function poly_combos(z, order, inds...)\n",
    "    if order == 0\n",
    "        return Num[1]\n",
    "    elseif order == length(inds)\n",
    "        return [_prod([z[i] for i in inds]...)]\n",
    "    else\n",
    "        start_ind = length(inds) == 0 ? 1 : inds[end]\n",
    "        return vcat([poly_combos(z, order, inds..., j) for j in start_ind:length(z)]...)\n",
    "    end\n",
    "end\n",
    "\n",
    "# gives all bases monomials up to a certain order\n",
    "function primal_monomial_basis(z, order::Int)\n",
    "    return Vector{Symbolics.Num}(vcat([poly_combos(z, i) for i in 1:order]...))\n",
    "end\n",
    "\n",
    "# calculates coefficient bases up to a certain order\n",
    "# mostly for use with trigonometric functions example sin(k*z),\n",
    "# where k is the coefficient\n",
    "function primal_coeff_basis(z, max_coeff::Int)\n",
    "    return Vector{Symbolics.Num}(vcat([k .* z for k in 1:max_coeff]...))\n",
    "end\n",
    "\n",
    "# calculates +,-,*,/ between states as a new basis\n",
    "# the return output is a set to avoid duplicates\n",
    "function primal_operator_basis(z, operator)\n",
    "    return Vector{Symbolics.Num}([operator(z[i], z[j]) for i in 1:length(z)-1 for j in i+1:length(z)] ∪ [operator(z[j], z[i]) for i in 1:length(z)-1 for j in i+1:length(z)])\n",
    "end\n",
    "\n",
    "function primal_power_basis(z, max_power::Int)\n",
    "    if max_power > 0\n",
    "        return Vector{Symbolics.Num}(vcat([z.^i for i in 1:max_power]...))\n",
    "    elseif max_power < 0\n",
    "        return Vector{Symbolics.Num}(vcat([z.^-i for i in 1:abs(max_power)]...))\n",
    "    end\n",
    "end\n",
    "\n",
    "function polynomial_basis(z::Vector{Symbolics.Num} = get_z_vector(2); polyorder::Int = 0, operator=nothing, max_coeff::Int = 0)\n",
    "    primes = primal_monomial_basis(z, polyorder)\n",
    "    primes = vcat(primes, primal_coeff_basis(z, max_coeff))\n",
    "    if operator !== nothing\n",
    "        primes = vcat(primes, primal_operator_basis(z, operator))\n",
    "    end\n",
    "    return primes\n",
    "end\n",
    "\n",
    "function trigonometric_basis(z::Vector{Symbolics.Num} = get_z_vector(2); polyorder::Int = 0, operator=nothing, max_coeff::Int = 0)\n",
    "    primes = polynomial_basis(z, polyorder = polyorder, operator = operator, max_coeff = max_coeff)\n",
    "    return vcat(sin.(primes), cos.(primes))\n",
    "end\n",
    "\n",
    "function exponential_basis(z::Vector{Symbolics.Num} = get_z_vector(2); polyorder::Int = 0, operator=nothing, max_coeff::Int = 0)\n",
    "    primes = polynomial_basis(z, polyorder = polyorder, operator = operator, max_coeff = max_coeff)\n",
    "    return exp.(primes)\n",
    "end\n",
    "\n",
    "function logarithmic_basis(z::Vector{Symbolics.Num} = get_z_vector(2); polyorder::Int = 0, operator=nothing, max_coeff::Int = 0)\n",
    "    primes = polynomial_basis(z, polyorder = polyorder, operator = operator, max_coeff = max_coeff)\n",
    "    return log.(abs.(primes))\n",
    "end\n",
    "\n",
    "function mixed_states_basis(basis::Vector{Symbolics.Num}...)\n",
    "    mixed_states = Tuple(basis)\n",
    "    \n",
    "    ham = Vector{Symbolics.Num}()\n",
    "    for i in eachindex(mixed_states)\n",
    "        for j in i+1:lastindex(mixed_states)\n",
    "            ham = vcat(ham, [mixed_states[i][k] * mixed_states[j][l] for k in 1:length(mixed_states[i]) for l in 1:length(mixed_states[j])])\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return Vector{Symbolics.Num}(ham)\n",
    "end\n",
    "\n",
    "# returns the number of required coefficients for the basis\n",
    "function get_numCoeffs(basis::Vector{Symbolics.Num})\n",
    "    return length(basis)\n",
    "end\n",
    "\n",
    "\n",
    "# gets a vector of combinations of hamiltonian basis\n",
    "function get_basis_set(basis::Vector{Symbolics.Num}...)\n",
    "    # gets a vector of combinations of basis\n",
    "    basis = vcat(basis...)\n",
    "    \n",
    "    # removes duplicates\n",
    "    basis = Vector{Symbolics.Num}(collect(unique(basis)))\n",
    "\n",
    "    return basis\n",
    "end\n",
    "\n",
    "# returns a function that can build the gradient of the hamiltonian\n",
    "function ΔH_func_builder(d::Int, z::Vector{Symbolics.Num} = get_z_vector(d), basis::Vector{Symbolics.Num}...) \n",
    "    # nd is the total number of dimensions of all the states, e.g. if q,p each of 3 dims, that is 6 dims in total\n",
    "    nd = 2d\n",
    "    Dz = Differential.(z)\n",
    "    \n",
    "    # collects and sums combinations of basis and coefficients\"\n",
    "    basis = get_basis_set(basis...)\n",
    "   \n",
    "    # gets number of terms in the basis\n",
    "    @variables a[1:get_numCoeffs(basis)]\n",
    "    \n",
    "    # collect and sum combinations of basis and coefficients\n",
    "    ham = sum(collect(a .* basis))\n",
    "    \n",
    "    # gives derivative of the hamiltonian, but not the skew-symmetric true one\n",
    "    f = [expand_derivatives(dz(ham)) for dz in Dz]\n",
    "\n",
    "    #simplify the expression potentially to make it faster\n",
    "    f = simplify(f)\n",
    "    \n",
    "    # line below makes the vector into a hamiltonian vector field by multiplying with the skew-symmetric matrix\n",
    "    ∇H = vcat(f[d+1:2d], -f[1:d])\n",
    "    \n",
    "    # builds a function that calculates Hamiltonian gradient and converts the function to a native Julia function\n",
    "    ∇H_eval = @RuntimeGeneratedFunction(Symbolics.inject_registered_module_functions(build_function(∇H, z, a)[2]))\n",
    "    \n",
    "    return ∇H_eval\n",
    "end\n",
    "\n",
    "struct HamiltonianSINDy{T, GHT}\n",
    "    basis::Vector{Symbolics.Num} # the augmented basis for sparsification\n",
    "    analytical_fθ::GHT\n",
    "    z::Vector{Symbolics.Num} \n",
    "    λ::T # Sparsification Parameter\n",
    "    noise_level::T # Noise amplitude added to the data\n",
    "    noiseGen_timeStep::T # Time step for the integrator to get noisy data \n",
    "    nloops::Int # Sparsification Loops\n",
    "    \n",
    "    function HamiltonianSINDy(basis::Vector{Symbolics.Num},\n",
    "        analytical_fθ::GHT = missing,\n",
    "        z::Vector{Symbolics.Num} = get_z_vector(2);\n",
    "        λ::T = 0.05,\n",
    "        noise_level::T = 0.00,\n",
    "        noiseGen_timeStep::T = 0.05,\n",
    "        nloops = 10) where {T, GHT <: Union{Base.Callable,Missing}}\n",
    "\n",
    "        new{T, GHT}(basis, analytical_fθ, z, λ, noise_level, noiseGen_timeStep, nloops)\n",
    "    end\n",
    "end\n",
    "\n",
    "function gen_noisy_ref_data(method::HamiltonianSINDy, x)\n",
    "    # initialize timestep data for analytical solution\n",
    "    tstep = method.noiseGen_timeStep\n",
    "    tspan = (zero(tstep), tstep)\n",
    "\n",
    "    function next_timestep(x)\n",
    "        prob_ref = ODEProblem((dx, t, x, params) -> method.analytical_fθ(dx, x, params, t), tspan, tstep, x)\n",
    "        sol = integrate(prob_ref, Gauss(2))\n",
    "        sol.q[end]\n",
    "    end\n",
    "\n",
    "    data_ref = [next_timestep(_x) for _x in x]\n",
    "\n",
    "    # add noise\n",
    "    data_ref_noisy = [_x .+ method.noise_level .* randn(size(_x)) for _x in data_ref]\n",
    "\n",
    "    return data_ref_noisy\n",
    "\n",
    "end\n",
    "\n",
    "struct TrainingData{AT<:AbstractArray}\n",
    "    x::AT # initial condition\n",
    "    ẋ::AT # initial condition\n",
    "    y::AT # noisy data at next time step\n",
    "\n",
    "    TrainingData(x::AT, ẋ::AT, y::AT) where {AT} = new{AT}(x, ẋ, y)\n",
    "    TrainingData(x::AT, ẋ::AT) where {AT} = new{AT}(x, ẋ)\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# Setup\n",
    "# --------------------\n",
    "\n",
    "println(\"Setting up...\")\n",
    "\n",
    "# 2D system with 4 variables [q₁, q₂, p₁, p₂]\n",
    "const nd = 4\n",
    "\n",
    "z = get_z_vector(nd/2)\n",
    "polynomial = polynomial_basis(z, polyorder=3)\n",
    "trigonometric  = trigonometric_basis(z, max_coeff=1)\n",
    "# prime_diff = primal_operator_basis(z, -)\n",
    "# basis = get_basis_set(polynomial, trigonometric, prime_diff)\n",
    "basis = get_basis_set(polynomial, trigonometric)\n",
    "# initialize analytical function, keep λ smaller than ϵ so system is identifiable\n",
    "ϵ = 0.5\n",
    "m = 1\n",
    "\n",
    "# two-dim simple harmonic oscillator (not used anywhere only in case some testing needed)\n",
    "# H_ana(x, p, t) = ϵ * x[1]^2 + ϵ * x[2]^2 + 1/(2*m) * x[3]^2 + 1/(2*m) * x[4]^2\n",
    "# H_ana(x, p, t) = cos(x[1]) + cos(x[2]) + 1/(2*m) * x[3]^2 + 1/(2*m) * x[4]^2\n",
    "\n",
    "# Gradient function of the 2D hamiltonian\n",
    "# grad_H_ana(x) = [x[3]; x[4]; -2ϵ * x[1]; -2ϵ * x[2]]\n",
    "grad_H_ana(x) = [x[3]; x[4]; sin(x[1]); sin(x[2])]\n",
    "function grad_H_ana!(dx, x, p, t)\n",
    "    dx .= grad_H_ana(x)\n",
    "end\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Training Data\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "println(\"Generate Training Data...\")\n",
    "\n",
    "# number of samples\n",
    "num_samp = 17\n",
    "\n",
    "# samples in p and q space\n",
    "samp_range = LinRange(-20, 20, num_samp)\n",
    "\n",
    "# initialize vector of matrices to store ODE solve output\n",
    "\n",
    "# s depend on size of nd (total dims), 4 in the case here so we use samp_range x samp_range x samp_range x samp_range\n",
    "s = collect(Iterators.product(fill(samp_range, nd)...))\n",
    "\n",
    "\n",
    "# compute vector field from x state values\n",
    "x = [collect(s[i]) for i in eachindex(s)]\n",
    "\n",
    "# normal_x = Flux.normalize(x)\n",
    "\n",
    "\n",
    "dx = zeros(nd)\n",
    "p = 0\n",
    "t = 0\n",
    "ẋ = [grad_H_ana!(copy(dx), _x, p, t) for _x in x]\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# Compute Sparse Regression\n",
    "# ----------------------------------------\n",
    "\n",
    "# choose SINDy method\n",
    "# (λ parameter must be close to noise value so that only coeffs with value around the noise are sparsified away)\n",
    "# noiseGen_timeStep chosen randomly for now\n",
    "method = HamiltonianSINDy(basis, grad_H_ana!, z, λ = 0.05, noise_level = 0.05, noiseGen_timeStep = 0.2)\n",
    "\n",
    "# generate noisy references data at next time step\n",
    "y = gen_noisy_ref_data(method, x)\n",
    "\n",
    "# collect training data\n",
    "tdata = TrainingData(x, ẋ, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimension of system\n",
    "d = size(tdata.x[begin], 1) ÷ 2\n",
    "\n",
    "# returns function that builds hamiltonian gradient through symbolics\n",
    "fθ = ΔH_func_builder(d, method.z, method.basis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder(x, model) =  (model[1].W  * x .+ model[1].b) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function SINDy(x₀, model, method, fθ)\n",
    "    \n",
    "    Δt = method.noiseGen_timeStep\n",
    "\n",
    "    numLoops = 8 # random choice of loop steps\n",
    "\n",
    "    # solution of SINDy Hamiltonian problem\n",
    "    local x̄ = zeros(eltype(model[2].W), axes(x₀))\n",
    "    local x̃ = zeros(eltype(model[2].W), axes(x₀))\n",
    "    local f = zeros(eltype(model[2].W), axes(x₀))\n",
    "\n",
    "    # gradient at current (x) values\n",
    "    fθ(f, x₀,  model[2].W)\n",
    "\n",
    "    # for first guess use explicit euler\n",
    "    x̃ .= x₀ .+ Δt .* f\n",
    "        \n",
    "    for _ in 1:numLoops\n",
    "        x̄ .= (x₀ .+ x̃) ./ 2\n",
    "        # find gradient at {(x̃ₙ + x̃ⁱₙ₊₁)/2} to get Hermite extrapolation\n",
    "        fθ(f, x̄, model[2].W)\n",
    "        # mid point rule for integration to next step\n",
    "        x̃ .= x₀ .+ Δt .* f\n",
    "    end\n",
    "    return x̃\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder(x, model) = (model[3].W * x .+ model[3].b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss_kernel is able to sparsify to the correct coefficients with the identity matrix but the coefficient values it generates are not correct. it did so with batches, no noise input, iterations = 30,800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function loss_kernel(xᵢₙ, x₁, ẋ, model, method, fθ)\n",
    "\n",
    "    local f = zeros(eltype(model[2].W), axes(xᵢₙ))\n",
    "    fθ(f, xᵢₙ, model[2].W)\n",
    "    L_grad = sqeuclidean(f, ẋ)\n",
    "\n",
    "    # calculate square Euclidean distance\n",
    "    # L_SINDy = sqeuclidean(SINDy(encoder(xᵢₙ, model), model, method, fθ), encoder(x₁, model))\n",
    "\n",
    "    # fθ(f, (encoder(xᵢₙ) .+ encoder(x₁)) ./ 2, model[2].W)\n",
    "    # L_SINDy = sqeuclidean(encoder(x₁) .- encoder(xᵢₙ) ./ method.noiseGen_timeStep, f)\n",
    "    \n",
    "    #Reconstruction loss from encoded-decoded xᵢₙ\n",
    "    # L_reconstruct = sqeuclidean(xᵢₙ, decoder(encoder(xᵢₙ, model), model)) #+ sqeuclidean(x₁, decoder(encoder(x₁, model), model))\n",
    "\n",
    "    # L_coeffs = sum(abs.(model[2].W))\n",
    "    # divided by 4000 to account for batch size. can use wrapper for loss instead to avoid this\n",
    "    # L_coeff_bool = 1/200 * sum(model[2].W .!= 0.0)\n",
    "\n",
    "    L_complete = sqeuclidean(decoder(SINDy(encoder(xᵢₙ, model), model, method, fθ), model), x₁)\n",
    "\n",
    "    return L_complete + L_grad\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function\n",
    "function loss(flattened_model::AbstractVector)\n",
    "    # Convert the flattened parameters back to the original structure\n",
    "    local recon_model = reconstruct_model(flattened_model, ld, ndim)\n",
    "    return mapreduce(z -> loss_kernel(z..., recon_model, method, fθ), +, zip(tdata.x, tdata.y, tdata.ẋ)) \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function flatten_model(model)\n",
    "    θ = [model[1].W, model[1].b, model[2].W, model[3].W, model[3].b]\n",
    "    # Flatten the model into a single vector\n",
    "    return flattened_model = cat([vec(θ[i]) for i in 1:length(θ)]..., dims=1)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function reconstruct_model(flattened_model,ld,ndim)\n",
    "    reconstructed_model = (\n",
    "        (W = reshape(flattened_model[1:ld*ndim], ld, ndim), b = reshape(flattened_model[(ld*ndim)+1:(ld*ndim)+ld], ld)),\n",
    "        (W = flattened_model[(ld*ndim)+ld+1:(ld*ndim)+ld+get_numCoeffs(method.basis)], ),\n",
    "        (W = reshape(flattened_model[(ld*ndim)+ld+get_numCoeffs(method.basis)+1:end-ld], ndim, ld), b = reshape(flattened_model[end-ld+1:end], ld))\n",
    "    )\n",
    "    return reconstructed_model\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latent dimension: ld\n",
    "ld = size(tdata.x[begin], 1)\n",
    "ndim = size(tdata.x[begin], 1)\n",
    "model = (\n",
    "\t(W = rand(ld, ndim), b = zeros(ld)),\n",
    "\t(W = zeros(get_numCoeffs(method.basis)), ),\n",
    "\t(W = rand(ndim, ld), b = zeros(ld)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_loss_array = Vector{Float64}()\n",
    "\n",
    "# initial guess\n",
    "println(\"Initial Guess...\")\n",
    "\n",
    "optimizer = Adam()\n",
    "\n",
    "batch_size = 8000  # Set your desired batch size\n",
    "num_batches = ceil(Int, length(tdata.x) / batch_size)\n",
    "\n",
    "flattened_model = flatten_model(model)\n",
    "@show model[2].W\n",
    "@show \"initial loss:\", loss(flattened_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random\n",
    "\n",
    "# val_loss_array = Float64[]\n",
    "\n",
    "for epoch in 1:7\n",
    "    # Shuffle the indices of the data\n",
    "    shuffled_indices = shuffle(1:length(tdata.x))\n",
    "    \n",
    "    # Training phase\n",
    "    train_loss = 0.0\n",
    "    for batch in 1:num_batches\n",
    "        # Get the indices for the current batch\n",
    "        batch_indices = shuffled_indices[(batch-1)*batch_size+1:min(batch*batch_size, length(tdata.x))]\n",
    "        \n",
    "        # Extract the current batch from tdata.x and tdata.y\n",
    "        batch_x = tdata.x[batch_indices]\n",
    "        batch_y = tdata.y[batch_indices]\n",
    "        batch_ẋ = tdata.ẋ[batch_indices]\n",
    "\n",
    "        # Define the loss function for the current batch\n",
    "        batch_loss(flattened_model::AbstractVector) = begin\n",
    "            local recon_model = reconstruct_model(flattened_model, ld, ndim)\n",
    "            mapreduce(z -> loss_kernel(z..., recon_model, method, fθ), +, zip(batch_x, batch_y, batch_ẋ))\n",
    "        end\n",
    "        \n",
    "        # Compute gradients using ForwardDiff.jl\n",
    "        gradients = ForwardDiff.gradient(batch_loss, flattened_model)\n",
    "\n",
    "        # Update the parameters using the optimizer\n",
    "        Flux.Optimise.update!(optimizer, flattened_model, gradients)\n",
    "        \n",
    "        push!(initial_loss_array, batch_loss(flattened_model))\n",
    "        if batch % 7 == 0\n",
    "            @show epoch, batch, batch_loss(flattened_model)\n",
    "            @show reconstruct_model(flattened_model, ld, ndim)[2].W  \n",
    "        end\n",
    "\n",
    "        train_loss += batch_loss(flattened_model)\n",
    "    end\n",
    "    train_loss /= num_batches\n",
    "\n",
    "    # Validation phase\n",
    "    # val_loss = 0.0\n",
    "    # num_val_batches = ceil(Int, length(x) / batch_size)\n",
    "    \n",
    "    # for val_batch_idx in 1:num_val_batches\n",
    "    #     # Get the indices for the current validation batch\n",
    "    #     val_batch_indices = shuffled_indices[(val_batch_idx-1)*batch_size+1:min(val_batch_idx*batch_size, length(x))]\n",
    "        \n",
    "    #     # Extract the current validation batch from the validation data\n",
    "    #     val_batch_x = x[val_batch_indices, :]\n",
    "    #     val_batch_y = original_y[val_batch_indices, :]\n",
    "        \n",
    "    #     # Define the loss function for the current batch\n",
    "    #     batch_val_loss(flattened_model::AbstractVector) = begin\n",
    "    #         local recon_model = reconstruct_model(flattened_model, ld, ndim)\n",
    "    #         mapreduce(z -> loss_kernel(z..., recon_model, method, fθ), +, zip(val_batch_x, val_batch_y))\n",
    "    #     end\n",
    "\n",
    "    #     # Compute the loss for the current validation batch\n",
    "    #     val_loss +=  batch_val_loss(flattened_model)\n",
    "    # end\n",
    "    \n",
    "    # val_loss /= num_val_batches\n",
    "    \n",
    "    # if  isempty(val_loss_array) || val_loss <= val_loss_array[end] \n",
    "        # push!(val_loss_array, val_loss)\n",
    "    # else\n",
    "    #     println(\"Val loss increased:\")\n",
    "    #     println(\"Epoch $epoch: Average Train Loss: $train_loss, Average Val Loss: $val_loss\")\n",
    "    #     break\n",
    "    # end\n",
    "    # Print the training and validation loss for the current epoch\n",
    "    println(\"Epoch $epoch: Average Train Loss: $train_loss\")#, Average Val Loss: $val_loss\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q0 = plot(log.(initial_loss_array), label = \"initial batch loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_model = reconstruct_model(flattened_model, ld, ndim)\n",
    "coeffs = reconstructed_model[2].W\n",
    "println(coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@show \"intermediate loss:\", loss(flattened_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_loss_array = Vector{Float64}()\n",
    "\n",
    "for n in 1:method.nloops\n",
    "    println(\"Iteration #$n...\")\n",
    "\n",
    "    # find coefficients below λ threshold\n",
    "    smallinds = abs.(coeffs) .< method.λ\n",
    "    biginds = .~smallinds\n",
    "    \n",
    "    # check if there are any small coefficients != 0 left\n",
    "    #TODO: is the code expected to exit the loop here usually?\n",
    "    all(coeffs[smallinds] .== 0) && break\n",
    "\n",
    "    # set all small coefficients to zero\n",
    "    coeffs[smallinds] .= 0\n",
    "\n",
    "    # Regress dynamics onto remaining terms to find sparse coeffs\n",
    "    function sparseloss(flattened_model::AbstractVector, batch_x, batch_y, batch_ẋ)\n",
    "        c = zeros(eltype(flattened_model), axes(coeffs))\n",
    "        c[biginds] .= flattened_model[(ld*ndim)+ld+1:end - ld*ndim - ld]\n",
    "\n",
    "        local reconstructed_model = (\n",
    "                (W = reshape(flattened_model[1:ld*ndim], ld, ndim), b = reshape(flattened_model[(ld*ndim)+1:(ld*ndim)+ld], ld)),\n",
    "                (W = c, ),\n",
    "                (W = reshape(flattened_model[end - ld*ndim - ld + 1 : end - ld], ndim, ld), b = reshape(flattened_model[end - ld + 1 : end], ld))\n",
    "        )\n",
    "        # Define the loss function for the current batch\n",
    "        batch_loss(flattened_model::AbstractVector) = begin\n",
    "            local recon_model = reconstruct_model(flattened_model, ld, ndim)\n",
    "            mapreduce(z -> loss_kernel(z..., recon_model, method, fθ), +, zip(batch_x, batch_y, batch_ẋ))\n",
    "        end\n",
    "        return batch_loss(flatten_model(reconstructed_model))\n",
    "    end\n",
    "\n",
    "    # θ is partly a reference to coeffs[biginds] so coeffs[biginds] will be updated\n",
    "    θ = [reconstructed_model[1].W, reconstructed_model[1].b, coeffs[biginds], reconstructed_model[3].W, reconstructed_model[3].b]\n",
    "    # Flatten the model into a single vector\n",
    "    flattened_model = cat([vec(θ[i]) for i in 1:length(θ)]..., dims=1)\n",
    "    \n",
    "    # Shuffle the indices for the batches\n",
    "    shuffled_indices = randperm(length(tdata.x))\n",
    "\n",
    "    for epoch in 1:100\n",
    "        for batch in 1:num_batches\n",
    "            # Get the indices for the current batch\n",
    "            start_index = (batch-1)*batch_size + 1\n",
    "            end_index = min(batch*batch_size, length(tdata.x))\n",
    "            batch_indices = shuffled_indices[start_index:end_index]\n",
    "            \n",
    "            # Extract the current batch from tdata.x and tdata.y\n",
    "            batch_x = tdata.x[batch_indices]\n",
    "            batch_y = tdata.y[batch_indices]\n",
    "            batch_ẋ = tdata.ẋ[batch_indices]\n",
    "\n",
    "            # Compute gradients using ForwardDiff.jl\n",
    "            gradients = ForwardDiff.gradient(flattened_model -> sparseloss(flattened_model, batch_x, batch_y, batch_ẋ), flattened_model)\n",
    "        \n",
    "            # Update the parameters using the optimizer\n",
    "            Flux.Optimise.update!(optimizer, flattened_model, gradients)\n",
    "            \n",
    "            push!(sparse_loss_array, sparseloss(flattened_model, batch_x, batch_y, batch_ẋ))\n",
    "\n",
    "            coeffs[biginds] = flattened_model[(ld*ndim)+ld+1:end - ld*ndim - ld]\n",
    "            \n",
    "            if epoch % 3 == 0 && batch % 3 == 0\n",
    "                @show epoch, batch, sparseloss(flattened_model, batch_x, batch_y, batch_ẋ) \n",
    "                @show coeffs  \n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    coeffs[biginds] = flattened_model[(ld*ndim)+ld+1:end - ld*ndim - ld]\n",
    "\n",
    "    reconstructed_model = (\n",
    "        (W = reshape(flattened_model[1:ld*ndim], ld, ndim), b = reshape(flattened_model[(ld*ndim)+1:(ld*ndim)+ld], ld)),\n",
    "        (W = coeffs, ),\n",
    "        (W = reshape(flattened_model[end - ld*ndim - ld + 1 : end - ld], ndim, ld), b = reshape(flattened_model[end - ld + 1 : end], ld))\n",
    "    )\n",
    "    \n",
    "    @show coeffs\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@show \"final loss:\", mapreduce(z -> loss_kernel(z..., reconstructed_model, method, fθ), +, zip(tdata.x, tdata.y, tdata.ẋ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p0 = plot((sparse_loss_array), label = \"sparse batch loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_model\n",
    "coeffs = reconstructed_model[2].W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function (vectorfield)(dz, z)\n",
    "    fθ(dz, z, reconstructed_model[2].W)\n",
    "    return dz\n",
    "end\n",
    "\n",
    "(vectorfield)(dz, z, p, t) = vectorfield(dz, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tstep = 0.01\n",
    "tspan = (0.0,20.0)\n",
    "\n",
    "prob_reference = ODEProblem((dx, t, x, params) -> grad_H_ana!(dx, x, params, t), tspan, tstep, x[50])\n",
    "data_reference = integrate(prob_reference, Gauss(1))\n",
    "\n",
    "prob_sindy = ODEProblem((dx, t, x, params) -> vectorfield(dx, x, params, t), tspan, tstep, x[50])\n",
    "data_sindy = integrate(prob_sindy, Gauss(1))\n",
    "\n",
    "p1 = plot(xlabel = \"Time\", ylabel = \"q₁\")\n",
    "scatter!(p1, data_reference.t, data_reference.q[:,1], label = \"Data q₁\")\n",
    "scatter!(p1, data_sindy.t, data_sindy.q[:,1], markershape=:xcross, label = \"Identified q₁\")\n",
    "\n",
    "p3 = plot(xlabel = \"Time\", ylabel = \"p₁\")\n",
    "scatter!(p3, data_reference.t, data_reference.q[:,3], label = \"Data p₁\")\n",
    "scatter!(p3, data_sindy.t, data_sindy.q[:,3], markershape=:xcross, label = \"Identified p₁\")\n",
    "\n",
    "plot!(size=(1000,1000))\n",
    "display(plot(p1, p3, title=\"Analytical vs Calculated q₁ & p₁ in a 2D system with Euler\"))\n",
    "\n",
    "\n",
    "\n",
    "# ref =  [decoder(data, reconstructed_model) for data in data_reference.q]\n",
    "# iden =  [decoder(data, reconstructed_model) for data in data_sindy.q]\n",
    "\n",
    "# ref = hcat(ref...)\n",
    "# iden = hcat(iden...)\n",
    "\n",
    "# p1 = plot(xlabel = \"Time\", ylabel = \"q₁\")\n",
    "# scatter!(p1, data_reference.t, ref[1,:], label = \"Data q₁\")\n",
    "# scatter!(p1, data_sindy.t, iden[1,:], markershape=:xcross, label = \"Identified q₁\")\n",
    "\n",
    "# p3 = plot(xlabel = \"Time\", ylabel = \"p₁\")\n",
    "# scatter!(p3, data_reference.t, ref[3,:], label = \"Data p₁\")\n",
    "# scatter!(p3, data_sindy.t, iden[3,:], markershape=:xcross, label = \"Identified p₁\")\n",
    "\n",
    "# plot!(size=(1000,1000))\n",
    "# display(plot(p1, p3, title=\"Analytical vs Calculated q₁ & p₁ in a 2D system with Euler\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------------------\n",
    "# Plot Results\n",
    "# ----------------------------------------\n",
    "\n",
    "println(\"Plotting...\")\n",
    "\n",
    "tstep = 0.01\n",
    "tspan = (0.0,15.0)\n",
    "\n",
    "for i in 1:5\n",
    "    idx = rand(1:length(s))\n",
    "\n",
    "    prob_reference = ODEProblem((dx, t, x, params) -> grad_H_ana!(dx, x, params, t), tspan, tstep, x[idx])\n",
    "    data_reference = integrate(prob_reference, Gauss(1))\n",
    "\n",
    "    prob_sindy = ODEProblem((dx, t, x, params) -> vectorfield(dx, x, params, t), tspan, tstep, x[idx])\n",
    "    data_sindy = integrate(prob_sindy, Gauss(1))\n",
    "\n",
    "    p1 = plot(xlabel = \"Time\", ylabel = \"q₁\")\n",
    "    scatter!(p1, data_reference.t, data_reference.q[:,1], label = \"Data q₁\")\n",
    "    scatter!(p1, data_sindy.t, data_sindy.q[:,1], markershape=:xcross, label = \"Identified q₁\")\n",
    "\n",
    "    p3 = plot(xlabel = \"Time\", ylabel = \"p₁\")\n",
    "    scatter!(p3, data_reference.t, data_reference.q[:,3], label = \"Data p₁\")\n",
    "    scatter!(p3, data_sindy.t, data_sindy.q[:,3], markershape=:xcross, label = \"Identified p₁\")\n",
    "\n",
    "    plot!(size=(1000,1000))\n",
    "    display(plot(p1, p3, title=\"Analytical vs Calculated q₁ & p₁ in a 2D system with Euler\"))\n",
    "\n",
    "    p2 = plot(xlabel = \"Time\", ylabel = \"q₂\")\n",
    "    scatter!(p2, data_reference.t, data_reference.q[:,2], label = \"Data q₂\")\n",
    "    scatter!(p2, data_sindy.t, data_sindy.q[:,2], markershape=:xcross, label = \"Identified q₂\")\n",
    "\n",
    "    p4 = plot(xlabel = \"Time\", ylabel = \"p₂\")\n",
    "    scatter!(p4, data_reference.t, data_reference.q[:,4], label = \"Data p₂\")\n",
    "    scatter!(p4, data_sindy.t, data_sindy.q[:,4], markershape=:xcross, label = \"Identified p₂\")\n",
    "\n",
    "    plot!(size=(1000,1000))\n",
    "    display(plot(p2, p4, title=\"Analytical vs Calculated q₂ & p₂ in a 2D system with Euler\"))\n",
    "\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.3",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
