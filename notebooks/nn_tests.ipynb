{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check number of threads being used first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_threads = Threads.nthreads()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code from Sparsification module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Distributions\n",
    "using GeometricIntegrators\n",
    "using Optim\n",
    "using Random\n",
    "using Flux\n",
    "using Distances\n",
    "using Symbolics\n",
    "using ForwardDiff\n",
    "using Plots\n",
    "using RuntimeGeneratedFunctions\n",
    "RuntimeGeneratedFunctions.init(@__MODULE__)\n",
    "\n",
    "gr()\n",
    "\n",
    "_prod(a, b, c, arrs...) = a .* _prod(b, c, arrs...)\n",
    "_prod(a, b) = a .* b\n",
    "_prod(a) = a\n",
    "\n",
    "# generates a vector out of symbolic arrays (p,q) with a certain dimension\n",
    "function get_z_vector(dims)\n",
    "    @variables q[1:dims]\n",
    "    @variables p[1:dims]\n",
    "    z = vcat(q,p)\n",
    "    return z\n",
    "end\n",
    "\n",
    "# make combinations of bases of just the order that is given \n",
    "# e.g order = 2 will give just the bases whose powers sum to 2\n",
    "function poly_combos(z, order, inds...)\n",
    "    if order == 0\n",
    "        return Num[1]\n",
    "    elseif order == length(inds)\n",
    "        return [_prod([z[i] for i in inds]...)]\n",
    "    else\n",
    "        start_ind = length(inds) == 0 ? 1 : inds[end]\n",
    "        return vcat([poly_combos(z, order, inds..., j) for j in start_ind:length(z)]...)\n",
    "    end\n",
    "end\n",
    "\n",
    "# gives all bases monomials up to a certain order\n",
    "function primal_monomial_basis(z, order::Int)\n",
    "    return Vector{Symbolics.Num}(vcat([poly_combos(z, i) for i in 1:order]...))\n",
    "end\n",
    "\n",
    "# calculates coefficient bases up to a certain order\n",
    "# mostly for use with trigonometric functions example sin(k*z),\n",
    "# where k is the coefficient\n",
    "function primal_coeff_basis(z, max_coeff::Int)\n",
    "    return Vector{Symbolics.Num}(vcat([k .* z for k in 1:max_coeff]...))\n",
    "end\n",
    "\n",
    "# calculates +,-,*,/ between states as a new basis\n",
    "# the return output is a set to avoid duplicates\n",
    "function primal_operator_basis(z, operator)\n",
    "    return Vector{Symbolics.Num}([operator(z[i], z[j]) for i in 1:length(z)-1 for j in i+1:length(z)] ∪ [operator(z[j], z[i]) for i in 1:length(z)-1 for j in i+1:length(z)])\n",
    "end\n",
    "\n",
    "function primal_power_basis(z, max_power::Int)\n",
    "    if max_power > 0\n",
    "        return Vector{Symbolics.Num}(vcat([z.^i for i in 1:max_power]...))\n",
    "    elseif max_power < 0\n",
    "        return Vector{Symbolics.Num}(vcat([z.^-i for i in 1:abs(max_power)]...))\n",
    "    end\n",
    "end\n",
    "\n",
    "function polynomial_basis(z::Vector{Symbolics.Num} = get_z_vector(2); polyorder::Int = 0, operator=nothing, max_coeff::Int = 0)\n",
    "    primes = primal_monomial_basis(z, polyorder)\n",
    "    primes = vcat(primes, primal_coeff_basis(z, max_coeff))\n",
    "    if operator !== nothing\n",
    "        primes = vcat(primes, primal_operator_basis(z, operator))\n",
    "    end\n",
    "    return primes\n",
    "end\n",
    "\n",
    "function trigonometric_basis(z::Vector{Symbolics.Num} = get_z_vector(2); polyorder::Int = 0, operator=nothing, max_coeff::Int = 0)\n",
    "    primes = polynomial_basis(z, polyorder = polyorder, operator = operator, max_coeff = max_coeff)\n",
    "    return vcat(sin.(primes), cos.(primes))\n",
    "end\n",
    "\n",
    "function exponential_basis(z::Vector{Symbolics.Num} = get_z_vector(2); polyorder::Int = 0, operator=nothing, max_coeff::Int = 0)\n",
    "    primes = polynomial_basis(z, polyorder = polyorder, operator = operator, max_coeff = max_coeff)\n",
    "    return exp.(primes)\n",
    "end\n",
    "\n",
    "function logarithmic_basis(z::Vector{Symbolics.Num} = get_z_vector(2); polyorder::Int = 0, operator=nothing, max_coeff::Int = 0)\n",
    "    primes = polynomial_basis(z, polyorder = polyorder, operator = operator, max_coeff = max_coeff)\n",
    "    return log.(abs.(primes))\n",
    "end\n",
    "\n",
    "function mixed_states_basis(basis::Vector{Symbolics.Num}...)\n",
    "    mixed_states = Tuple(basis)\n",
    "    \n",
    "    ham = Vector{Symbolics.Num}()\n",
    "    for i in eachindex(mixed_states)\n",
    "        for j in i+1:lastindex(mixed_states)\n",
    "            ham = vcat(ham, [mixed_states[i][k] * mixed_states[j][l] for k in 1:length(mixed_states[i]) for l in 1:length(mixed_states[j])])\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return Vector{Symbolics.Num}(ham)\n",
    "end\n",
    "\n",
    "# returns the number of required coefficients for the basis\n",
    "function get_numCoeffs(basis::Vector{Symbolics.Num})\n",
    "    return length(basis)\n",
    "end\n",
    "\n",
    "\n",
    "# gets a vector of combinations of hamiltonian basis\n",
    "function get_basis_set(basis::Vector{Symbolics.Num}...)\n",
    "    # gets a vector of combinations of basis\n",
    "    basis = vcat(basis...)\n",
    "    \n",
    "    # removes duplicates\n",
    "    basis = Vector{Symbolics.Num}(collect(unique(basis)))\n",
    "\n",
    "    return basis\n",
    "end\n",
    "\n",
    "# returns a function that can build the gradient of the hamiltonian\n",
    "function ΔH_func_builder(d::Int, z::Vector{Symbolics.Num} = get_z_vector(d), basis::Vector{Symbolics.Num}...) \n",
    "    # nd is the total number of dimensions of all the states, e.g. if q,p each of 3 dims, that is 6 dims in total\n",
    "    nd = 2d\n",
    "    Dz = Differential.(z)\n",
    "    \n",
    "    # collects and sums combinations of basis and coefficients\"\n",
    "    basis = get_basis_set(basis...)\n",
    "   \n",
    "    # gets number of terms in the basis\n",
    "    @variables a[1:get_numCoeffs(basis)]\n",
    "    \n",
    "    # collect and sum combinations of basis and coefficients\n",
    "    ham = sum(collect(a .* basis))\n",
    "    \n",
    "    # gives derivative of the hamiltonian, but not the skew-symmetric true one\n",
    "    f = [expand_derivatives(dz(ham)) for dz in Dz]\n",
    "\n",
    "    #simplify the expression potentially to make it faster\n",
    "    f = simplify(f)\n",
    "    \n",
    "    # line below makes the vector into a hamiltonian vector field by multiplying with the skew-symmetric matrix\n",
    "    ∇H = vcat(f[d+1:2d], -f[1:d])\n",
    "    \n",
    "    # builds a function that calculates Hamiltonian gradient and converts the function to a native Julia function\n",
    "    ∇H_eval = @RuntimeGeneratedFunction(Symbolics.inject_registered_module_functions(build_function(∇H, z, a)[2]))\n",
    "    \n",
    "    return ∇H_eval\n",
    "end\n",
    "\n",
    "struct HamiltonianSINDy{T, GHT}\n",
    "    basis::Vector{Symbolics.Num} # the augmented basis for sparsification\n",
    "    analytical_fθ::GHT\n",
    "    z::Vector{Symbolics.Num} \n",
    "    λ::T # Sparsification Parameter\n",
    "    noise_level::T # Noise amplitude added to the data\n",
    "    noiseGen_timeStep::T # Time step for the integrator to get noisy data \n",
    "    nloops::Int # Sparsification Loops\n",
    "    \n",
    "    function HamiltonianSINDy(basis::Vector{Symbolics.Num},\n",
    "        analytical_fθ::GHT = missing,\n",
    "        z::Vector{Symbolics.Num} = get_z_vector(2);\n",
    "        λ::T = 0.05,\n",
    "        noise_level::T = 0.00,\n",
    "        noiseGen_timeStep::T = 0.05,\n",
    "        nloops = 10) where {T, GHT <: Union{Base.Callable,Missing}}\n",
    "\n",
    "        new{T, GHT}(basis, analytical_fθ, z, λ, noise_level, noiseGen_timeStep, nloops)\n",
    "    end\n",
    "end\n",
    "\n",
    "function gen_noisy_ref_data(method::HamiltonianSINDy, x)\n",
    "    # initialize timestep data for analytical solution\n",
    "    tstep = method.noiseGen_timeStep\n",
    "    tspan = (zero(tstep), tstep)\n",
    "\n",
    "    function next_timestep(x)\n",
    "        prob_ref = ODEProblem((dx, t, x, params) -> method.analytical_fθ(dx, x, params, t), tspan, tstep, x)\n",
    "        sol = integrate(prob_ref, Gauss(2))\n",
    "        sol.q[end]\n",
    "    end\n",
    "\n",
    "    data_ref = [next_timestep(_x) for _x in x]\n",
    "\n",
    "    # add noise\n",
    "    data_ref_noisy = [_x .+ method.noise_level .* randn(size(_x)) for _x in data_ref]\n",
    "\n",
    "    return data_ref_noisy\n",
    "\n",
    "end\n",
    "\n",
    "struct TrainingData{AT<:AbstractArray}\n",
    "    x::AT # initial condition\n",
    "    ẋ::AT # initial condition\n",
    "    y::AT # noisy data at next time step\n",
    "\n",
    "    TrainingData(x::AT, ẋ::AT, y::AT) where {AT} = new{AT}(x, ẋ, y)\n",
    "    TrainingData(x::AT, ẋ::AT) where {AT} = new{AT}(x, ẋ)\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up...\n",
      "Generate Training Data..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainingData{Vector{Vector{Float64}}}([[-20.0, -20.0, -20.0, -20.0], [-17.647058823529413, -20.0, -20.0, -20.0], [-15.294117647058824, -20.0, -20.0, -20.0], [-12.941176470588234, -20.0, -20.0, -20.0], [-10.588235294117645, -20.0, -20.0, -20.0], [-8.235294117647058, -20.0, -20.0, -20.0], [-5.882352941176468, -20.0, -20.0, -20.0], [-3.529411764705884, -20.0, -20.0, -20.0], [-1.1764705882352935, -20.0, -20.0, -20.0], [1.1764705882352935, -20.0, -20.0, -20.0]  …  [-1.1764705882352935, 20.0, 20.0, 20.0], [1.1764705882352935, 20.0, 20.0, 20.0], [3.529411764705884, 20.0, 20.0, 20.0], [5.882352941176471, 20.0, 20.0, 20.0], [8.235294117647062, 20.0, 20.0, 20.0], [10.588235294117645, 20.0, 20.0, 20.0], [12.941176470588232, 20.0, 20.0, 20.0], [15.294117647058824, 20.0, 20.0, 20.0], [17.647058823529413, 20.0, 20.0, 20.0], [20.0, 20.0, 20.0, 20.0]], [[-20.0, -20.0, -0.9129452507276277, -0.9129452507276277], [-20.0, -20.0, 0.9329410219916074, -0.9129452507276277], [-20.0, -20.0, -0.40213327081314787, -0.9129452507276277], [-20.0, -20.0, -0.36609186992584053, -0.9129452507276277], [-20.0, -20.0, 0.9181782765546844, -0.9129452507276277], [-20.0, -20.0, -0.9281770087913974, -0.9129452507276277], [-20.0, -20.0, 0.39018486717736567, -0.9129452507276277], [-20.0, -20.0, 0.3781703886946707, -0.9129452507276277], [-20.0, -20.0, -0.9232558158568607, -0.9129452507276277], [-20.0, -20.0, 0.9232558158568607, -0.9129452507276277]  …  [20.0, 20.0, -0.9232558158568607, 0.9129452507276277], [20.0, 20.0, 0.9232558158568607, 0.9129452507276277], [20.0, 20.0, -0.3781703886946707, 0.9129452507276277], [20.0, 20.0, -0.39018486717736317, 0.9129452507276277], [20.0, 20.0, 0.9281770087913961, 0.9129452507276277], [20.0, 20.0, -0.9181782765546844, 0.9129452507276277], [20.0, 20.0, 0.36609186992583886, 0.9129452507276277], [20.0, 20.0, 0.40213327081314787, 0.9129452507276277], [20.0, 20.0, -0.9329410219916074, 0.9129452507276277], [20.0, 20.0, 0.9129452507276277, 0.9129452507276277]], [[-21.00121076471114, -21.00121076471114, -20.047772627129874, -20.047772627129874], [-18.646127595720742, -21.00121076471114, -19.969015713882545, -20.047772627129874], [-16.29421916647702, -21.00121076471114, -19.995872423722375, -20.047772627129874], [-13.94196438666524, -21.00121076471114, -20.036788143370057, -20.047772627129874], [-11.587023083818707, -21.00121076471114, -19.952267769380864, -20.047772627129874], [-9.236214727087575, -21.00121076471114, -20.030476103730475, -20.047772627129874], [-6.882267076297052, -21.00121076471114, -20.00474247106978, -20.047772627129874], [-4.528611985838396, -21.00121076471114, -19.96282484774511, -20.047772627129874], [-2.1776837851323165, -21.00121076471114, -20.047656691841485, -20.047772627129874], [0.177381154973685, -21.00121076471114, -19.96997830870247, -20.047772627129874]  …  [-0.177381154973685, 21.00121076471114, 19.96997830870247, 20.047772627129874], [2.1776837851323165, 21.00121076471114, 20.047656691841485, 20.047772627129874], [4.528611985838396, 21.00121076471114, 19.96282484774511, 20.047772627129874], [6.882267076297055, 21.00121076471114, 20.00474247106978, 20.047772627129874], [9.236214727087578, 21.00121076471114, 20.030476103730475, 20.047772627129874], [11.587023083818707, 21.00121076471114, 19.952267769380864, 20.047772627129874], [13.941964386665239, 21.00121076471114, 20.036788143370057, 20.047772627129874], [16.29421916647702, 21.00121076471114, 19.995872423722375, 20.047772627129874], [18.646127595720742, 21.00121076471114, 19.969015713882545, 20.047772627129874], [21.00121076471114, 21.00121076471114, 20.047772627129874, 20.047772627129874]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --------------------\n",
    "# Setup\n",
    "# --------------------\n",
    "\n",
    "println(\"Setting up...\")\n",
    "\n",
    "# 2D system with 4 variables [q₁, q₂, p₁, p₂]\n",
    "const nd = 4\n",
    "\n",
    "z = get_z_vector(nd/2)\n",
    "polynomial = polynomial_basis(z, polyorder=3)\n",
    "trigonometric  = trigonometric_basis(z, max_coeff=1)\n",
    "prime_diff = primal_operator_basis(z, -)\n",
    "basis = get_basis_set(polynomial, trigonometric, prime_diff)\n",
    "# initialize analytical function, keep λ smaller than ϵ so system is identifiable\n",
    "ϵ = 0.5\n",
    "m = 1\n",
    "\n",
    "# two-dim simple harmonic oscillator (not used anywhere only in case some testing needed)\n",
    "# H_ana(x, p, t) = ϵ * x[1]^2 + ϵ * x[2]^2 + 1/(2*m) * x[3]^2 + 1/(2*m) * x[4]^2\n",
    "# H_ana(x, p, t) = cos(x[1]) + cos(x[2]) + 1/(2*m) * x[3]^2 + 1/(2*m) * x[4]^2\n",
    "\n",
    "# Gradient function of the 2D hamiltonian\n",
    "# grad_H_ana(x) = [x[3]; x[4]; -2ϵ * x[1]; -2ϵ * x[2]]\n",
    "grad_H_ana(x) = [x[3]; x[4]; sin(x[1]); sin(x[2])]\n",
    "function grad_H_ana!(dx, x, p, t)\n",
    "    dx .= grad_H_ana(x)\n",
    "end\n",
    "# ------------------------------------------------------------\n",
    "# Training Data\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "println(\"Generate Training Data...\")\n",
    "\n",
    "# number of samples\n",
    "num_samp = 18\n",
    "\n",
    "# samples in p and q space\n",
    "samp_range = LinRange(-20, 20, num_samp)\n",
    "\n",
    "# initialize vector of matrices to store ODE solve output\n",
    "\n",
    "# s depend on size of nd (total dims), 4 in the case here so we use samp_range x samp_range x samp_range x samp_range\n",
    "s = collect(Iterators.product(fill(samp_range, nd)...))\n",
    "\n",
    "\n",
    "# compute vector field from x state values\n",
    "x = [collect(s[i]) for i in eachindex(s)]\n",
    "dx = zeros(nd)\n",
    "p = 0\n",
    "t = 0\n",
    "ẋ = [grad_H_ana!(copy(dx), _x, p, t) for _x in x]\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# Compute Sparse Regression\n",
    "# ----------------------------------------\n",
    "\n",
    "# choose SINDy method\n",
    "# (λ parameter must be close to noise value so that only coeffs with value around the noise are sparsified away)\n",
    "# noiseGen_timeStep chosen randomly for now\n",
    "method = HamiltonianSINDy(basis, grad_H_ana!, z, λ = 0.05, noise_level = 0.0, noiseGen_timeStep = 0.05) #noise_level = 0.6\n",
    "\n",
    "# generate noisy references data at next time step\n",
    "y = gen_noisy_ref_data(method, x)\n",
    "\n",
    "# collect training data\n",
    "tdata = TrainingData(x, ẋ, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimension of system\n",
    "d = size(tdata.x[begin], 1) ÷ 2\n",
    "\n",
    "# returns function that builds hamiltonian gradient through symbolics\n",
    "fθ = ΔH_func_builder(d, method.z, method.basis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "network_two (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# evaluate neural network\n",
    "function network_two(xᵢₙ, model, method, fθ)\n",
    "\t# input layer\n",
    "\t# input_layer = relu.(model[1].W  * xᵢₙ)\n",
    "\n",
    "\t# first hidden layer\n",
    "\t# x₀ = relu.(model[2].W * input_layer .+ model[2].b)\n",
    "    x₀ = leakyrelu.(model[1].W  * xᵢₙ .+ model[1].b, 0.05) #rrelu\n",
    "\t# SINDy layer\n",
    "\tfunction SINDy_layer()\n",
    "        # coeffs initialized to a vector of zeros b/c easier to optimize zeros for our case\n",
    "        coeffs = model[2].W\n",
    "\n",
    "        numLoops = 4 # random choice of loop steps\n",
    "        \n",
    "        local x̄ = zeros(eltype(coeffs), axes(x₀))\n",
    "        local x̃ = zeros(eltype(coeffs), axes(x₀))\n",
    "        local f = zeros(eltype(coeffs), axes(x₀))\n",
    "\n",
    "        # gradient at current (x) values\n",
    "        fθ(f, x₀, coeffs)\n",
    "\n",
    "        # for first guess use explicit euler\n",
    "        x̃ .= x₀ .+ method.noiseGen_timeStep .* f\n",
    "        \n",
    "        for _ in 1:numLoops\n",
    "            x̄ .= (x₀ .+ x̃) ./ 2\n",
    "            # find gradient at {(x̃ₙ + x̃ⁱₙ₊₁)/2} to get Hermite extrapolation\n",
    "            fθ(f, x̄, coeffs)\n",
    "            # mid point rule for integration to next step\n",
    "            x̃ .= x₀ .+ method.noiseGen_timeStep .* f\n",
    "        end\n",
    "        return x̃\n",
    "    end\n",
    "    # return  SINDy_layer()\n",
    "\t# output layer (linear activation)\n",
    "\treturn (model[3].W * SINDy_layer())\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function loss_kernel(xᵢₙ, x₁, model, method, fθ)\n",
    "    x̃₁ = network_two(xᵢₙ, model, method, fθ)\n",
    "    # calculate square Euclidean distance\n",
    "    sqeuclidean(x̃₁, x₁)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function\n",
    "function loss(flattened_model::AbstractVector)\n",
    "    # λ_l2 = 0.01  # L2 regularization strength\n",
    "    # λ_l1 = 0.01  # L1 regularization strength\n",
    "    # reg_loss = λ_l2 * sum(abs2, flattened_model) + λ_l1 * sum(abs, flattened_model)\n",
    "    # Convert the flattened parameters back to the original structure\n",
    "    local recon_model = reconstruct_model(flattened_model, ld, ndim)\n",
    "    # return reg_loss + mapreduce(z -> loss_kernel(z..., recon_model, method, fθ), +, zip(tdata.x, tdata.y)) \n",
    "    return mapreduce(z -> loss_kernel(z..., recon_model, method, fθ), +, zip(tdata.x, tdata.y)) \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function flatten_model(model)\n",
    "    θ = [model[1].W, model[1].b, model[2].W, model[3].W]\n",
    "    # Flatten the model into a single vector\n",
    "    return flattened_model = cat([vec(θ[i]) for i in 1:length(θ)]..., dims=1)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function reconstruct_model(flattened_model,ld,ndim)\n",
    "    reconstructed_model = (\n",
    "        (W = reshape(flattened_model[1:ld*ndim], ld, ndim), b = reshape(flattened_model[(ld*ndim)+1:(ld*ndim)+ld], ld)),\n",
    "        (W = flattened_model[(ld*ndim)+ld+1:(ld*ndim)+ld+get_numCoeffs(method.basis)], ),\n",
    "        (W = reshape(flattened_model[(ld*ndim)+ld+get_numCoeffs(method.basis)+1:end], ndim, ld), ),\n",
    "    )\n",
    "    return reconstructed_model\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((W = [1.0 0.0 0.0 0.0; 0.0 1.0 0.0 0.0; 0.0 0.0 1.0 0.0; 0.0 0.0 0.0 1.0], b = [0.0, 0.0, 0.0, 0.0]), (W = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],), (W = [1.0 0.0 0.0 0.0; 0.0 1.0 0.0 0.0; 0.0 0.0 1.0 0.0; 0.0 0.0 0.0 1.0],))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using LinearAlgebra\n",
    "# latent dimension: ld\n",
    "ld = size(tdata.x[begin], 1)\n",
    "ndim = size(tdata.x[begin], 1)\n",
    "model = (\n",
    "\t(W = Matrix{Float64}(I, ld, ndim), b = zeros(ld)),   #(W = randn(ld, ndim), b = randn(ld)),\n",
    "\t(W = zeros(get_numCoeffs(method.basis)), ),\n",
    "\t(W = Matrix{Float64}(I, ndim, ld), ) #(W = randn(ndim, ld), ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial_loss_array = Vector{Float64}()\n",
    "\n",
    "# # initial guess\n",
    "# println(\"Initial Guess...\")\n",
    "# flattened_model = flatten_model(model)\n",
    "# # Define the optimization solver\n",
    "# solver = BFGS()\n",
    "# result = Optim.optimize(loss, flattened_model, solver, Optim.Options(show_trace=true); autodiff = :forward)\n",
    "# @show loss(flattened_model)\n",
    "# push!(initial_loss_array, loss(flattened_model))\n",
    "\n",
    "# reconstructed_model = reconstruct_model(result.minimizer, ld, ndim)\n",
    "\n",
    "# coeffs = reconstructed_model[2].W\n",
    "\n",
    "# (result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Guess...\n",
      "((reconstruct_model(flattened_model, ld, ndim))[2]).W = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(i, loss(flattened_model)) = (1, 2.782880207830313e7)\n",
      "((reconstruct_model(flattened_model, ld, ndim))[2]).W = [0.003162277660144041, 0.0031622776601440404, -0.0031622776601548217, -0.0031622776601548225, 0.0031622776601659564, 0.0031622776601659577, -0.0031622776284495296, -8.730579757534373e-8, 0.003162277660165956, -8.953324221477148e-8, -0.0031622776284495296, -0.003162277660166784, -0.00316227766016703, -0.0031622776601667834, 0.0031622776601682684, 0.0031622776601681834, -0.0031622776601666156, -0.003162277660167816, 0.0031622776601681825, 0.0031622776601674153, 0.0031622776601674158, 0.003162277660166803, -0.0031622776601678416, 0.0031622776601676157, 0.003162277660168269, -0.003162277660167816, -0.0031622776601666156, 0.0031622776601676157, -0.003162277660167842, 0.0031622776601668025, -0.0031622776601683052, -0.003162277660168262, -0.003162277660168262, -0.0031622776601683052, 0.0031622776601122815, 0.0031622776601122815, -0.0031622776601394165, -0.003162277660139416, 0.0031622776600421444, 0.003162277660042144, -0.003162277660108791, -0.0031622776601087908, -9.00206720847243e-8, 0.0031622776601562103, 0.0031622776601562103, 0.0031622776601562103, 0.0031622776601562107, -8.993578208280783e-8, -8.996132971509574e-8, -0.003162277660161601, -0.003162277660161601, -0.0031622776601616014, -0.0031622776601616014, -9.004621971701222e-8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(i, loss(flattened_model)) = (2, 2.7199848951465715e7)\n",
      "((reconstruct_model(flattened_model, ld, ndim))[2]).W = [0.007408453603005455, 0.007408453603015612, -0.007408278192401519, -0.0074082781924061075, 0.007406463576585843, 0.007406848435686307, -0.00015102232871083698, 0.003162190351848481, 0.007406463576609946, 0.003162188124391338, -0.00015102231809578084, -0.007404841540703518, -0.007406748262960502, -0.007404841540727163, 0.007405893257818845, 0.007405464464691378, -0.007401466127606603, -0.007410659641195426, 0.007405464464194832, 0.007402838690399171, 0.007402838689746017, 0.0074110687514692825, -0.007401495423021355, 0.007411868666930553, 0.007405893257843247, -0.007410659640066992, -0.007401466127145006, 0.007411868664240476, -0.007401495425502385, 0.007411068751559634, -0.007404178244711495, -0.007404827957662775, -0.007404827957410426, -0.007404178244734112, 0.007407837572774234, 0.007407837572810807, -0.0074076495680381275, -0.007407649568053008, 0.00741291179190277, 0.007412911791834958, -0.007413277619292961, -0.0074132776192446515, -3.913172858001329e-6, 0.007408366131509914, 0.0074083661315122, 0.007408366131515028, 0.007408366131517339, 1.5034121086947762e-6, 3.571211584459654e-6, -0.0074083661315187815, -0.00740836613152107, -0.007408366131523898, -0.007408366131526209, -1.8453773010505597e-6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(i, loss(flattened_model)) = (3, 2.650674928551151e7)\n",
      "((reconstruct_model(flattened_model, ld, ndim))[2]).W = [0.01234575036381865, 0.012345750326682964, -0.012345364113133278, -0.012345364096368574, 0.01233241853309117, 0.012337631594880726, 0.004012344044026266, 0.005998646961147538, 0.012332418478428626, 0.005998434939174112, 0.004012341206681038, -0.012331998517128635, -0.012337783630398672, -0.012331998470295378, 0.01232922870958654, 0.012330468966339587, -0.012353752852333696, -0.01236478598860299, 0.01233046897007262, 0.012302659058642258, 0.01230265903261359, 0.012355436407462929, -0.012308956225367885, 0.012360027595567724, 0.012329228655683817, -0.012364786040474529, -0.012353752925241688, 0.012360027507560139, -0.012308956266630085, 0.012355436315607631, -0.012329810729069546, -0.01232860674739969, -0.012328606740971424, -0.012329810691857328, 0.01234456573994001, 0.012344565685743662, -0.012340477981724544, -0.012340477916778562, 0.012368259573826151, 0.01236825959910913, -0.01236484273077015, -0.012364842840857933, 0.0029395258108617334, 0.012345557692181913, 0.012345557683810564, 0.012345557673589724, 0.012345557665218438, -0.0023841444944730926, -0.002939920983498332, -0.012345557692193379, -0.012345557683822034, -0.012345557673601196, -0.012345557665229906, 0.002383706745501813]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(i, loss(flattened_model)) = (4, 2.5795710069570545e7)\n",
      "((reconstruct_model(flattened_model, ld, ndim))[2]).W = [0.017756976439167585, 0.01775701588683826, -0.017756481828334465, -0.017756513910604222, 0.017711860596049553, 0.017733927474975514, 0.008854406347681896, 0.009333573065823837, 0.017711929415019593, 0.009469782638872383, 0.008859063881187132, -0.01772168149872703, -0.017732775255282735, -0.01772175039328596, 0.017701357164498423, 0.017708283133221966, -0.017819692832407603, -0.017814801797066897, 0.017708279937334023, 0.017628893946560354, 0.017628891269406425, 0.01778002313675114, -0.017634935099224425, 0.017788131013540508, 0.017701423231177557, -0.01781471424074182, -0.017819688918820797, 0.017788239649532613, -0.017634909696962277, 0.017780072720076274, -0.017716194289275056, -0.01770737963871376, -0.017707381010010386, -0.017716254471320088, 0.01775667539757096, 0.01775674653511422, -0.0177409056912321, -0.017740986133610897, 0.017821861341500305, 0.01782180377746749, -0.01780429961546297, -0.01780420031657616, -0.00021920225062999066, 0.017756729765848307, 0.017756745807898076, 0.01775674949303692, 0.017756765531048067, 0.000776166651591078, 0.0002188070138762774, -0.017756729765861838, -0.01775674580791161, -0.017756749493050457, -0.017756765531061598, -0.0007766044794583846]\n"
     ]
    }
   ],
   "source": [
    "initial_loss_array = Vector{Float64}()\n",
    "\n",
    "# initial guess\n",
    "println(\"Initial Guess...\")\n",
    "\n",
    "flattened_model = flatten_model(model)\n",
    "@show reconstruct_model(flattened_model, ld, ndim)[2].W\n",
    "\n",
    "optimizer = AMSGrad()#AdamW()\n",
    "\n",
    "for i in 1:60\n",
    "    # Compute gradients using ForwardDiff.jl\n",
    "    gradients = ForwardDiff.gradient(loss, flattened_model)\n",
    "\n",
    "    # Update the parameters using the optimizer\n",
    "    Flux.Optimise.update!(optimizer, flattened_model, gradients)\n",
    "    push!(initial_loss_array, loss(flattened_model))\n",
    "    @show i, loss(flattened_model)\n",
    "    @show reconstruct_model(flattened_model, ld, ndim)[2].W\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial_loss_array = Vector{Float64}()\n",
    "# initial_i = Vector{Float64}()\n",
    "# initial_j = Vector{Float64}()\n",
    "\n",
    "# # initial guess\n",
    "# println(\"Initial Guess...\")\n",
    "\n",
    "# flattened_model = flatten_model(model)\n",
    "# @show reconstruct_model(flattened_model, ld, ndim)[2].W\n",
    "\n",
    "# optimizer = NAdam()\n",
    "\n",
    "# batch_size = 1000  # Set your desired batch size\n",
    "\n",
    "# num_batches = ceil(Int, length(tdata.x) / batch_size)\n",
    "\n",
    "# for i in 1:50\n",
    "#     for j in 1:num_batches\n",
    "#         # Get the indices for the current batch\n",
    "#         batch_indices = (j-1)*batch_size+1:min(j*batch_size, length(tdata.x))\n",
    "        \n",
    "#         # Extract the current batch from tdata.x and tdata.y\n",
    "#         batch_x = tdata.x[batch_indices]\n",
    "#         batch_y = tdata.y[batch_indices]\n",
    "        \n",
    "#         # Define the loss function for the current batch\n",
    "#         batch_loss(flattened_model::AbstractVector) = begin\n",
    "#             local recon_model = reconstruct_model(flattened_model, ld, ndim)\n",
    "#             mapreduce(z -> loss_kernel(z..., recon_model, method, fθ), +, zip(batch_x, batch_y))\n",
    "#         end\n",
    "        \n",
    "#         # Compute gradients using ForwardDiff.jl\n",
    "#         gradients = ForwardDiff.gradient(batch_loss, flattened_model)\n",
    "\n",
    "#         # Update the parameters using the optimizer\n",
    "#         Flux.Optimise.update!(optimizer, flattened_model, gradients)\n",
    "        \n",
    "#         push!(initial_loss_array, batch_loss(flattened_model))\n",
    "#         push!(initial_i, i)\n",
    "#         push!(initial_j, j)\n",
    "#         if i % 2 == 0 && j % 20 == 0\n",
    "#             @show i, j, batch_loss(flattened_model)\n",
    "#             @show reconstruct_model(flattened_model, ld, ndim)[2].W\n",
    "#         end\n",
    "#     end\n",
    "# end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q0 = plot(initial_loss_array, label = \"initial batch loss\")\n",
    "# plot!(q0, log.(initial_i), label = \"i index\")\n",
    "# plot!(q0, log.(initial_j), label = \"j index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_model = reconstruct_model(flattened_model, ld, ndim)\n",
    "coeffs = reconstructed_model[2].W\n",
    "println(coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_loss_array = Vector{Float64}()\n",
    "\n",
    "for n in 1:method.nloops\n",
    "    println(\"Iteration #$n...\")\n",
    "\n",
    "    # find coefficients below λ threshold\n",
    "    smallinds = abs.(coeffs) .< method.λ\n",
    "    biginds = .~smallinds\n",
    "    \n",
    "    # check if there are any small coefficients != 0 left\n",
    "    #TODO: is the code expected to exit the loop here usually?\n",
    "    all(coeffs[smallinds] .== 0) && break\n",
    "\n",
    "    # set all small coefficients to zero\n",
    "    coeffs[smallinds] .= 0\n",
    "\n",
    "    # Regress dynamics onto remaining terms to find sparse coeffs\n",
    "    function sparseloss(flattened_model::AbstractVector)\n",
    "        c = zeros(eltype(flattened_model), axes(coeffs))\n",
    "        c[biginds] .= flattened_model[(ld*ndim)+ld+1:end - ld*ndim]\n",
    "\n",
    "        local reconstructed_model = ((W = reshape(flattened_model[1:ld*ndim], ld, ndim), b = reshape(flattened_model[(ld*ndim)+1:(ld*ndim)+ld], ld)),\n",
    "            (W = c, ),\n",
    "            (W = reshape(flattened_model[end - ld*ndim + 1:end], ndim, ld), ),\n",
    "        )\n",
    "        return loss(flatten_model(reconstructed_model))\n",
    "    end\n",
    "\n",
    "    # θ is partly a reference to coeffs[biginds] so coeffs[biginds] will be updated\n",
    "    θ = [reconstructed_model[1].W, reconstructed_model[1].b, coeffs[biginds], reconstructed_model[3].W]\n",
    "    # Flatten the model into a single vector\n",
    "    flattened_model = cat([vec(θ[i]) for i in 1:length(θ)]..., dims=1)\n",
    "    \n",
    "    for i in 1:600\n",
    "        # Compute gradients using ForwardDiff.jl\n",
    "        gradients = ForwardDiff.gradient(sparseloss, flattened_model)\n",
    "    \n",
    "        # Update the parameters using the optimizer\n",
    "        Flux.Optimise.update!(optimizer, flattened_model, gradients)\n",
    "        push!(sparse_loss_array, sparseloss(flattened_model))\n",
    "        @show i, sparseloss(flattened_model)\n",
    "    end\n",
    "\n",
    "    coeffs[biginds] = flattened_model[(ld*ndim)+ld+1:end - ld*ndim]\n",
    "\n",
    "    reconstructed_model = ((W = reshape(flattened_model[1:ld*ndim], ld, ndim), b = reshape(flattened_model[(ld*ndim)+1:(ld*ndim)+ld], ld)),\n",
    "            (W = coeffs, ),\n",
    "            (W = reshape(flattened_model[end - ld*ndim + 1:end], ndim, ld), ),\n",
    "        )\n",
    "    @show coeffs\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = ADAM(0.000005)\n",
    "# sparse_loss_array = Vector{Float64}()\n",
    "# sparse_i = Vector{Float64}()\n",
    "# sparse_j = Vector{Float64}()\n",
    "\n",
    "\n",
    "# for n in 1:method.nloops\n",
    "#     println(\"Iteration #$n...\")\n",
    "\n",
    "#     # find coefficients below λ threshold\n",
    "#     smallinds = abs.(coeffs) .< method.λ\n",
    "#     biginds = .~smallinds\n",
    "    \n",
    "#     # check if there are any small coefficients != 0 left\n",
    "#     #TODO: is the code expected to exit the loop here usually?\n",
    "#     all(coeffs[smallinds] .== 0) && break\n",
    "\n",
    "#     # set all small coefficients to zero\n",
    "#     coeffs[smallinds] .= 0\n",
    "\n",
    "#     # Regress dynamics onto remaining terms to find sparse coeffs\n",
    "#     function sparseloss(flattened_model::AbstractVector, batch_x, batch_y)\n",
    "#         c = zeros(eltype(flattened_model), axes(coeffs))\n",
    "#         c[biginds] .= flattened_model[(ld*ndim)+ld+1:end - ld*ndim]\n",
    "\n",
    "#         local reconstructed_model = ((W = reshape(flattened_model[1:ld*ndim], ld, ndim), b = reshape(flattened_model[(ld*ndim)+1:(ld*ndim)+ld], ld)),\n",
    "#             (W = c, ),\n",
    "#             (W = reshape(flattened_model[end - ld*ndim + 1:end], ndim, ld), ),\n",
    "#         )\n",
    "#         # Define the loss function for the current batch\n",
    "#         batch_loss(flattened_model::AbstractVector) = begin\n",
    "#             local recon_model = reconstruct_model(flattened_model, ld, ndim)\n",
    "#             mapreduce(z -> loss_kernel(z..., recon_model, method, fθ), +, zip(batch_x, batch_y))\n",
    "#         end\n",
    "#         return batch_loss(flatten_model(reconstructed_model))\n",
    "#     end\n",
    "\n",
    "#     # θ is partly a reference to coeffs[biginds] so coeffs[biginds] will be updated\n",
    "#     θ = [reconstructed_model[1].W, reconstructed_model[1].b, coeffs[biginds], reconstructed_model[3].W]\n",
    "#     # Flatten the model into a single vector\n",
    "#     flattened_model = cat([vec(θ[i]) for i in 1:length(θ)]..., dims=1)\n",
    "    \n",
    "#     for i in 1:100\n",
    "#         for j in 1:num_batches\n",
    "#             # Get the indices for the current batch\n",
    "#             batch_indices = (j-1)*batch_size+1:min(j*batch_size, length(tdata.x))\n",
    "            \n",
    "#             # Extract the current batch from tdata.x and tdata.y\n",
    "#             batch_x = tdata.x[batch_indices]\n",
    "#             batch_y = tdata.y[batch_indices]\n",
    "            \n",
    "#             # Compute gradients using ForwardDiff.jl\n",
    "#             gradients = ForwardDiff.gradient(flattened_model -> sparseloss(flattened_model, batch_x, batch_y), flattened_model)\n",
    "        \n",
    "#             # Update the parameters using the optimizer\n",
    "#             Flux.Optimise.update!(optimizer, flattened_model, gradients)\n",
    "            \n",
    "#             if i % 10 == 0 && j % 100 == 0\n",
    "#                 push!(sparse_loss_array, sparseloss(flattened_model, batch_x, batch_y))\n",
    "#                 push!(sparse_i, i)\n",
    "#                 push!(sparse_j, j)\n",
    "#                 @show sparseloss(flattened_model, batch_x, batch_y)\n",
    "#             end\n",
    "#         end\n",
    "#     end\n",
    "\n",
    "#     coeffs[biginds] = flattened_model[(ld*ndim)+ld+1:end - ld*ndim]\n",
    "\n",
    "#     reconstructed_model = ((W = reshape(flattened_model[1:ld*ndim], ld, ndim), b = reshape(flattened_model[(ld*ndim)+1:(ld*ndim)+ld], ld)),\n",
    "#             (W = coeffs, ),\n",
    "#             (W = reshape(flattened_model[end - ld*ndim + 1:end], ndim, ld), ),\n",
    "#         )\n",
    "#     @show coeffs\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p0 = plot(log.(sparse_loss_array))\n",
    "# plot!(p0, log.(sparse_i), label = \"i index\")\n",
    "# plot!(p0, log.(sparse_j), label = \"j index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs = reconstructed_model[2].W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function (vectorfield)(dz, z)\n",
    "    fθ(dz, z, reconstructed_model[2].W)\n",
    "    return dz\n",
    "end\n",
    "\n",
    "(vectorfield)(dz, z, p, t) = vectorfield(dz, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------------------\n",
    "# Plot Results\n",
    "# ----------------------------------------\n",
    "\n",
    "println(\"Plotting...\")\n",
    "\n",
    "tstep = 0.01\n",
    "tspan = (0.0,25.0)\n",
    "\n",
    "for i in 1:5\n",
    "    idx = rand(1:length(s))\n",
    "\n",
    "    prob_reference = ODEProblem((dx, t, x, params) -> grad_H_ana!(dx, x, params, t), tspan, tstep, x[idx])\n",
    "    data_reference = integrate(prob_reference, Gauss(1))\n",
    "\n",
    "    prob_sindy = ODEProblem((dx, t, x, params) -> vectorfield(dx, x, params, t), tspan, tstep, x[idx])\n",
    "    data_sindy = integrate(prob_sindy, Gauss(1))\n",
    "\n",
    "    p1 = plot(xlabel = \"Time\", ylabel = \"q₁\")\n",
    "    scatter!(p1, data_reference.t, data_reference.q[:,1], label = \"Data q₁\")\n",
    "    scatter!(p1, data_sindy.t, data_sindy.q[:,1], markershape=:xcross, label = \"Identified q₁\")\n",
    "\n",
    "    p3 = plot(xlabel = \"Time\", ylabel = \"p₁\")\n",
    "    scatter!(p3, data_reference.t, data_reference.q[:,3], label = \"Data p₁\")\n",
    "    scatter!(p3, data_sindy.t, data_sindy.q[:,3], markershape=:xcross, label = \"Identified p₁\")\n",
    "\n",
    "    plot!(size=(1000,1000))\n",
    "    display(plot(p1, p3, title=\"Analytical vs Calculated q₁ & p₁ in a 2D system with Euler\"))\n",
    "\n",
    "    p2 = plot(xlabel = \"Time\", ylabel = \"q₂\")\n",
    "    scatter!(p2, data_reference.t, data_reference.q[:,2], label = \"Data q₂\")\n",
    "    scatter!(p2, data_sindy.t, data_sindy.q[:,2], markershape=:xcross, label = \"Identified q₂\")\n",
    "\n",
    "    p4 = plot(xlabel = \"Time\", ylabel = \"p₂\")\n",
    "    scatter!(p4, data_reference.t, data_reference.q[:,4], label = \"Data p₂\")\n",
    "    scatter!(p4, data_sindy.t, data_sindy.q[:,4], markershape=:xcross, label = \"Identified p₂\")\n",
    "\n",
    "    plot!(size=(1000,1000))\n",
    "    display(plot(p2, p4, title=\"Analytical vs Calculated q₂ & p₂ in a 2D system with Euler\"))\n",
    "\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.3",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
